{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import nltk\n",
    "from time import sleep\n",
    "# from tqdm import tqdm\n",
    "\n",
    "train_file = open(\"train.json\",'r')\n",
    "dev_file = open(\"dev.json\",'r')\n",
    "test_file=open(\"test.json\",'r')\n",
    "train = json.loads(train_file.read())\n",
    "dev = json.loads(dev_file.read())\n",
    "test = json.loads(test_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from six import iteritems\n",
    "from six.moves import xrange\n",
    "\n",
    "# BM25 parameters.\n",
    "PARAM_K1 = 0.0\n",
    "PARAM_B = 1.0\n",
    "EPSILON = 0.0\n",
    "\n",
    "class BM25(object):\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus_size = len(corpus)\n",
    "        self.avgdl = sum(map(lambda x: float(len(x)), corpus)) / self.corpus_size\n",
    "        self.corpus = corpus\n",
    "        self.f = []\n",
    "        self.df = {}\n",
    "        self.idf = {}\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for document in self.corpus:\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.f.append(frequencies)\n",
    "\n",
    "            for word, freq in iteritems(frequencies):\n",
    "                if word not in self.df:\n",
    "                    self.df[word] = 0\n",
    "                self.df[word] += 1\n",
    "\n",
    "        for word, freq in iteritems(self.df):\n",
    "            self.idf[word] = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "\n",
    "    def get_score(self, document, index, average_idf):\n",
    "        score = 0\n",
    "        for word in document:\n",
    "            if word not in self.f[index]:\n",
    "                continue\n",
    "            idf = self.idf[word] if self.idf[word] >= 0 else EPSILON * average_idf\n",
    "            score += (idf * self.f[index][word] * (PARAM_K1 + 1)\n",
    "                      / (self.f[index][word] + PARAM_K1 * (1 - PARAM_B + PARAM_B * self.corpus_size / self.avgdl)))\n",
    "        return score\n",
    "\n",
    "    def get_scores(self, document, average_idf):\n",
    "        scores = []\n",
    "        for index in xrange(self.corpus_size):\n",
    "            score = self.get_score(document, index, average_idf)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def get_bm25_weights(corpus):\n",
    "    bm25 = BM25(corpus)\n",
    "    average_idf = sum(map(lambda k: float(bm25.idf[k]), bm25.idf.keys())) / len(bm25.idf.keys())\n",
    "\n",
    "    weights = []\n",
    "    for doc in corpus:\n",
    "        scores = bm25.get_scores(doc, average_idf)\n",
    "        weights.append(scores)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "from nltk import StanfordPOSTagger\n",
    "\n",
    "stanford_tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def lemmas_count(word,synset):\n",
    "    lemmas = synset.lemmas()\n",
    "    lemma_names = synset.lemma_names()\n",
    "    count = 0\n",
    "    for i in range(len(lemma_names)):\n",
    "        if lemma_names[i].lower() == word:\n",
    "            count += lemmas[i].count()\n",
    "    return count\n",
    "\n",
    "def most_freq_synset(word,synsets):\n",
    "    freq_count = None\n",
    "    mini = 0\n",
    "    for synset in synsets:\n",
    "        if mini < lemmas_count(word,synset):\n",
    "            mini = lemmas_count(word,synset)\n",
    "            freq_count = synset\n",
    "    try:\n",
    "        if freq_count == None:\n",
    "            return word\n",
    "    except:\n",
    "        return str(freq_count).split(\"'\")[1].split(\".\")[0]\n",
    "    return word\n",
    "\n",
    "def tokenization(sent):\n",
    "    word_dict_sent = []\n",
    "    try:\n",
    "        token = stanford_tagger.tag(word_tokenizer.tokenize(sent.decode('utf-8')))\n",
    "    except:\n",
    "        token = stanford_tagger.tag(word_tokenizer.tokenize(sent))\n",
    "    \n",
    "    for word,tag in token:\n",
    "        word = lemmatize(stemmer.stem(word.lower()))\n",
    "        if tag in [\"JJ\",\"JJR\",\"JJS\"]:\n",
    "            word = most_freq_synset(word,wn.synsets(word,pos=wn.ADJ))\n",
    "            word_dict_sent.append(word)\n",
    "        elif tag in [\"RB\",\"RBR\",\"RBS\"]:\n",
    "            word =  most_freq_synset(word,wn.synsets(word,pos=wn.ADV))\n",
    "            word_dict_sent.append(word)\n",
    "        elif tag in [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]:\n",
    "            word = most_freq_synset(word,wn.synsets(word,pos=wn.NOUN))\n",
    "            word_dict_sent.append(word)\n",
    "        elif tag in [\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]:\n",
    "            word = most_freq_synset(word,wn.synsets(word,pos=wn.VERB))\n",
    "            word_dict_sent.append(word)\n",
    "        elif tag in [\"IN\",\"CD\",\"TO\"]:\n",
    "            word_dict_sent.append(word)\n",
    "    return word_dict_sent\n",
    "\n",
    "\n",
    "def get_corpus(doc):\n",
    "    corpus = []\n",
    "    for sent in doc:\n",
    "        corpus.append(tokenization(sent))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_tfidf = [] \n",
    "\n",
    "for doc in dev:\n",
    "    bm25Model = BM25(get_corpus(doc[\"sentences\"]))\n",
    "    average_idf = sum(map(lambda k: float(bm25Model.idf[k]), bm25Model.idf.keys())) / len(bm25Model.idf.keys())\n",
    "    question_tfidf_doc = []\n",
    "    for ques in doc[\"qa\"]:\n",
    "        scores = bm25Model.get_scores(tokenization(ques[\"question\"]), average_idf)\n",
    "        question_tfidf_doc.append(scores)\n",
    "    question_tfidf.append(question_tfidf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import copy\n",
    "question_index_rank = copy.deepcopy(question_tfidf)\n",
    "\n",
    "for i in range(len(question_index_rank)):\n",
    "    for j in range(len(question_index_rank[i])):\n",
    "        question = question_index_rank[i][j]\n",
    "        rank = {}\n",
    "        for index in range(len(question)):\n",
    "            rank[index] = question[index]\n",
    "        question = sorted(rank.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        question_index_rank[i][j] = []\n",
    "        for index,value in question:\n",
    "            question_index_rank[i][j].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_best_doc_num2(query):\n",
    "#     query =  transformer.transform(vectorizer.transform(get_BOW(query)))\n",
    "#     result={}\n",
    "#     for x in range(term_matrix.shape[0]):\n",
    "#          result[x]=cos_distance(query.toarray(),term_matrix[x].toarray())\n",
    "            \n",
    "#     minvalue=1\n",
    "#     first=0\n",
    "#     for item in result:\n",
    "#         if minvalue > result[item]:\n",
    "#             minvalue=result[item]\n",
    "#             first=item     \n",
    "#     del result[first]\n",
    "    \n",
    "#     minvalue=1\n",
    "#     second=0\n",
    "#     for item in result:\n",
    "#         if minvalue > result[item]:\n",
    "#             minvalue=result[item]\n",
    "#             second=item     \n",
    "#     return first,second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from scipy.spatial.distance import cosine as cos_distance\n",
    "\n",
    "# vectorizer = DictVectorizer()\n",
    "# transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "# # store guessed question sentence no\n",
    "# match_sent= [] #[[(best_match_sent_no,second_match_sent_no),...][second doc]]\n",
    "# count = 0\n",
    "\n",
    "# for dev_doc in tqdm(dev, desc='Extracting sentences from documents'):\n",
    "#     count += 1\n",
    "#     doc_match_sent = []\n",
    "#     term_matrix = transformer.fit_transform(vectorizer.fit_transform(cal_BOW(dev_doc)))\n",
    "#     for qa in dev_doc['qa']:\n",
    "#         doc_match_sent.append(get_best_doc_num2(qa['question']))\n",
    "#     match_sent.append(doc_match_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cor_sent = []\n",
    "# cor_num = 0\n",
    "# total = 0\n",
    "# for i in range(len(dev)):\n",
    "#     cor_per_doc = []\n",
    "#     for j in range(len(dev[i][\"qa\"])):\n",
    "#         cor_per_doc.append(dev[i][\"qa\"][j][\"answer_sentence\"])\n",
    "# #         total += 1\n",
    "#         if dev[i][\"qa\"][j][\"answer_sentence\"] in question_index_rank[i][j][:1]:\n",
    "#             cor_num += 1\n",
    "#     cor_sent.append(cor_per_doc)\n",
    "# print (cor_num+0.0)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_n(question_index_rank,n):\n",
    "    match_sent = []\n",
    "    for i in range(len(question_index_rank)):\n",
    "        match_sent.append([])\n",
    "        for j in range(len(question_index_rank[i])):\n",
    "            match_sent[i].append(question_index_rank[i][j][:n])\n",
    "    return match_sent\n",
    "\n",
    "match_sent = first_n(question_index_rank,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('stanford-ner-2016-10-31/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "               'stanford-ner-2016-10-31/stanford-ner.jar') \n",
    "# st = StanfordNERTagger('/Users/Luna/Downloads/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "#                '/Users/Luna/Downloads/stanford-ner/stanford-ner.jar') \n",
    "\n",
    "# tokenize sentence\n",
    "ner_tag_1 = []\n",
    "for i in range(len(dev)):\n",
    "    doc_tag = []\n",
    "    for j in range(len(dev[i]['sentences'])):\n",
    "        sentence = dev[i]['sentences'][j]\n",
    "        if '\"' in sentence:\n",
    "            sentence = sentence.replace('\"',\"\")\n",
    "        if sentence[:-1]==\".\":\n",
    "            sentence = sentence[:-1]\n",
    "        try:\n",
    "            doc_tag.append(word_tokenizer.tokenize(sentence.decode(\"utf-8\")))\n",
    "        except:\n",
    "            doc_tag.append(word_tokenizer.tokenize(sentence))\n",
    "            \n",
    "    doc_tag = st.tag_sents(doc_tag)\n",
    "    ner_tag_1.append(doc_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_list_discard = [\"(\",\")\",\"[\",\"]\"]\n",
    "symbol_list = [\",\",\"-\",\"/\",\".\",u'\\u2013',u'\\u002C',u'\\u002E',u'\\u2215']\n",
    "\n",
    "def hasSymbol(inputString):\n",
    "    \n",
    "    for char in symbol_list_discard:\n",
    "        if char in inputString:\n",
    "            return False\n",
    "    for char in symbol_list:\n",
    "        if char in inputString:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    if any((char.isdigit() or ('$' in char)) for char in inputString):\n",
    "        return True\n",
    "    else:\n",
    "        for word in word_tokenizer.tokenize(inputString):\n",
    "            if word in ['one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
    "                  'eight', 'nine', 'eleven', 'twelve', 'thirteen',\n",
    "                  'fourteen', 'fifteen', 'sixteen',\n",
    "                  'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty',\n",
    "                  'seventy', 'eighty', 'ninety', 'seventeen', 'eighteen',\n",
    "                  'nineteen',\n",
    "                  'nm','Hz','millions','million','hundred',u'\\xb0C',u'\\xb0F']:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_same_word(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-2:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                term_n2,tag_n2 = tag_list[k][i][j+2]\n",
    "                tmp = term+term_n+term_n2\n",
    "                \n",
    "                if hasSymbol(tmp) and tmp in dev[k][\"sentences\"][i]:\n",
    "                    if tag_n2 != 'O':\n",
    "                        temp =  (tmp,tag_n2)\n",
    "                    elif tag_n != 'O':\n",
    "                        temp =  (tmp,tag_n)\n",
    "                    elif tag != 'O':\n",
    "                        temp = (tmp,tag)\n",
    "                    else:\n",
    "                        temp = (tmp,\"OTHER\")\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+2]\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 1\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_entity(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-1:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                tmp = term+term_n\n",
    "                \n",
    "                if tag == tag_n and tag != \"O\" and term_n!=\",\" and term_n !=\";\":\n",
    "                    if tmp in dev[k][\"sentences\"][i]:\n",
    "                        temp =  (tmp,tag)\n",
    "                    else:\n",
    "                        temp =  (term + \" \" + term_n,tag)\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 1\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn O and ORGANIZATION into OTHER; digit into NUMBER\n",
    "\n",
    "def tune_other_and_number(tag_list):\n",
    "    for i in range(len(tag_list)): # each document\n",
    "        for j in range(len(tag_list[i])): # each sentence\n",
    "            for k in range(len(tag_list[i][j])): # each question\n",
    "                term,tag = tag_list[i][j][k] \n",
    "                if term!='' and len(term)>0 and (term,tag)!=tag_list[i][j][0] and tag == 'O' and term[0].isupper():\n",
    "                    tag_list[i][j][k] = (term,\"OTHER\")\n",
    "                if  hasNumbers(term) and (tag == \"O\" or tag ==\"OTHER\"):\n",
    "                    tag_list[i][j][k] = (term,\"NUMBER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopword = [\"of\"]\n",
    "tags = [\"LOCATION\",\"ORGANIZATION\",\"OTHER\"]\n",
    "\n",
    "def link_stopword(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-2:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                term_n2,tag_n2 = tag_list[k][i][j+2]\n",
    "                tmp = term+\" \"+term_n+\" \"+term_n2\n",
    "                \n",
    "                if term_n in stopword and (tag in tags and tag_n2 in tags):\n",
    "                    if tag_n2 != 'OTHER':\n",
    "                        temp =  (tmp,tag_n2)\n",
    "                    elif tag != 'OTHER':\n",
    "                        temp = (tmp,tag)\n",
    "                    else:\n",
    "                        temp = (tmp,\"OTHER\")\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+2]\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 1\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "ner_tag = []\n",
    "ner_tag = copy.deepcopy(ner_tag_1)\n",
    "\n",
    "combine_same_word(ner_tag)\n",
    "combine_entity(ner_tag)\n",
    "tune_other_and_number(ner_tag)\n",
    "combine_entity(ner_tag)\n",
    "link_stopword(ner_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = [\"PERSON\",\"LOCATION\",\"NUMBER\",\"OTHER\",\"ORGANIZATION\",\"PERCENT\",\"DATE\"]\n",
    "\n",
    "def create_entity(first,second):\n",
    "    sent_tag_dict = dict.fromkeys(tags,[])\n",
    "    for k in [first]:\n",
    "        for j in ner_tag[i][k]:\n",
    "            term,tag = j\n",
    "            if term =='' or len(term) == 1:\n",
    "                continue\n",
    "            if tag in tags:\n",
    "                sent_tag_dict[tag] = sent_tag_dict[tag]+ [term]\n",
    "            elif tag == \"TIME\":\n",
    "                sent_tag_dict[\"NUMBER\"] = sent_tag_dict[\"NUMBER\"]+ [term]\n",
    "    for tag in tags:\n",
    "        sent_tag_dict[tag] = list(set(sent_tag_dict[tag]))\n",
    "    return sent_tag_dict\n",
    "\n",
    "def remove_tag(list_tag):\n",
    "    list_tmp = []\n",
    "    for tup in list_tag:\n",
    "        term,tag = tup\n",
    "        if term != '':\n",
    "            list_tmp.append(term)\n",
    "    return list_tmp\n",
    "def remove_tag(list_tag):\n",
    "    list_tmp = []\n",
    "    for tup in list_tag:\n",
    "        term,tag = tup\n",
    "        if term != '':\n",
    "            list_tmp.append(term)\n",
    "    return list_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_sent_list = []\n",
    "entity_pool = []\n",
    "\n",
    "for i in range(len(match_sent)):\n",
    "    doc = dev[i]\n",
    "    sent_pool = doc['sentences']\n",
    "    test_list_doc = []\n",
    "    entity_pool_doc = []\n",
    "    \n",
    "    for ques in match_sent[i]:\n",
    "        test_list_sent = []\n",
    "        entity_pool_sent = []\n",
    "        for index in ques:\n",
    "            test_list_sent.append(remove_tag(ner_tag[i][index]))\n",
    "            entity_pool_sent.append(create_entity(index,i))\n",
    "        test_list_doc.append(test_list_sent)\n",
    "        entity_pool_doc.append(entity_pool_sent)\n",
    "        \n",
    "    question_sent_list.append(test_list_doc)\n",
    "    entity_pool.append(entity_pool_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVC\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "vectorizer = DictVectorizer()\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "import ast\n",
    "\n",
    "def train():\n",
    "    f1 = open('output.csv')\n",
    "    csv1 = csv.DictReader(f1)\n",
    "    texts = []\n",
    "    target = []\n",
    "\n",
    "    for item in csv1:\n",
    "\n",
    "        BOW = {}\n",
    "        try:\n",
    "            question = item[\"new\"]\n",
    "            questionx = ast.literal_eval(question)\n",
    "            # print x[0]\n",
    "            # print type(x)\n",
    "            if questionx == [] or questionx == None:\n",
    "                continue\n",
    "            for word in questionx:\n",
    "                BOW[word] = BOW.get(word, 0) + 1\n",
    "            texts.append(BOW)\n",
    "            target.append(item['Type'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    brown_matrix = vectorizer.fit_transform(texts).toarray()\n",
    "    svm.fit(brown_matrix, target)\n",
    "train()\n",
    "def predict(sentenct):\n",
    "    BOW = {}\n",
    "    for word in word_tokenizer.tokenize(sentenct):\n",
    "        word=lemmatize(word)\n",
    "        BOW[word] = BOW.get(word, 0) + 1\n",
    "#     print vectorizer.transform([BOW])\n",
    "    sentenctlist=vectorizer.transform([BOW]).toarray()\n",
    "    return svm.predict(sentenctlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_ranked_ans(entities_dic, question, sentence_token):\n",
    "    # identify if the entity set is empty. If True, return nothing\n",
    "    tmp_rank = {}\n",
    "    is_empty = True\n",
    "   \n",
    "    for values in entities_dic.values():\n",
    "        if len(values) != 0:\n",
    "            is_empty = False\n",
    "            \n",
    "    q_type = predict(question)[0]\n",
    "    if is_empty == False:\n",
    "        resultTpye={\"PERSON\":{},\"LOCATION\":{},\"NUMBER\":{},\"OTHER\":{},\"ORGANIZATION\":{},\"PERCENT\":{},\"DATE\":{}}\n",
    "        entitiesList=[]\n",
    "        for ent_type,entities in entities_dic.items():\n",
    "            for entity in entities:\n",
    "                resultTpye[ent_type][entity]=0\n",
    "                if (entity.lower() in question.lower()) or (entity.lower().replace('-', ' ') in question.lower()):\n",
    "                    resultTpye[ent_type][entity] -= 999\n",
    "                entitiesList.append(entity)\n",
    "                \n",
    "        preferred_entity = get_preferred_entity_list(entitiesList, sentence_token, question)\n",
    "        if not preferred_entity==None and not preferred_entity== {}:\n",
    "          \n",
    "            maxvalue=max([i for i in preferred_entity.values()]) \n",
    "            if maxvalue!=0:\n",
    "                for ent_type in resultTpye:\n",
    "                    for word in resultTpye[ent_type]:\n",
    "                        try:\n",
    "                            resultTpye[ent_type][word] += round(preferred_entity[word]*1.00,2)/maxvalue\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "        if q_type =='PERSON':\n",
    "            resultTpye=setWeight(resultTpye,[1,0.4,0.2,0.4,0.8,0.2,0.2])\n",
    "                \n",
    "        elif q_type =='ORGANIZATION':\n",
    "            resultTpye=setWeight(resultTpye,[0.9,0.5,0.2,0.5,1,0.2,0.2])\n",
    "        \n",
    "        elif q_type =='LOCATION':\n",
    "            resultTpye=setWeight(resultTpye,[0.8,1,0.2,0.5,0.7,0.2,0.2])\n",
    "                \n",
    "        elif q_type =='PERCENT':\n",
    "            resultTpye=setWeight(resultTpye,[0.8,0.5,0.8,0.4,0.7,1,0.6])\n",
    "                \n",
    "        elif q_type =='DATE':\n",
    "            \n",
    "            resultTpye=setWeight(resultTpye,[0.8,0.5,0.5,0.5,0.7,0.7,1])\n",
    "            for item in resultTpye['DATE']:\n",
    "                if bool(re.match('(?<!\\d)\\d{4}[s]?(?!\\d)', item)) and ('year' in question):\n",
    "                    resultTpye['DATE'][item]+=1\n",
    "                    \n",
    "        elif q_type =='NUMBER':\n",
    "            resultTpye=setWeight(resultTpye,[0.8,0.5,1,0.5,0.7,0.5,0.5])\n",
    "            if ('percentage' in question) or ('percent' in question):\n",
    "                 for item in resultTpye['PERCENT']:\n",
    "                    resultTpye['PERCENT'][item]+=1\n",
    "        \n",
    "        # sort and choose the best answer\n",
    "        sorted_ans = sorted(tmp_rank.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        # log for error analysis\n",
    "#         output_file.write('Q_type: ' + '\\t' + q_type + '\\n')\n",
    "#         output_file.write('Ranked Answers: ' + '\\t' + str(resultTpye) + '\\n\\n')\n",
    "      \n",
    "        best=0\n",
    "        best_ans=''\n",
    "        for ent_type in resultTpye:\n",
    "            for word in resultTpye[ent_type]:\n",
    "                if resultTpye[ent_type][word] > best:\n",
    "                    best_ans=word\n",
    "                    best=resultTpye[ent_type][word]\n",
    "        \n",
    "        if q_type =='DATE' and ('year' in question):\n",
    "            regex = re.compile(r'(?<!\\d)\\d{4}[s]?(?!\\d)')\n",
    "            year = regex.findall(best_ans)\n",
    "            if year != []:\n",
    "                best_ans = year[0]\n",
    "                        \n",
    "        return best_ans, best, entitiesList\n",
    "    else:\n",
    "        return '', 0, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setWeight(resultTpye,weight):\n",
    "            for item in resultTpye['PERSON']:\n",
    "                resultTpye['PERSON'][item]+=weight[0]\n",
    "            for item in resultTpye['LOCATION']:\n",
    "                resultTpye['LOCATION'][item]+=weight[1]\n",
    "            for item in resultTpye['NUMBER']:\n",
    "                resultTpye['NUMBER'][item]+=weight[2]\n",
    "            for item in resultTpye['OTHER']:\n",
    "                resultTpye['OTHER'][item]+=weight[3]\n",
    "            for item in resultTpye['ORGANIZATION']:\n",
    "                resultTpye['ORGANIZATION'][item]+=weight[4]\n",
    "            for item in resultTpye['PERCENT']:\n",
    "                resultTpye['PERCENT'][item]+=weight[5]\n",
    "            for item in resultTpye['DATE']:\n",
    "                resultTpye['DATE'][item]+=weight[6]\n",
    "            return resultTpye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_question_type(question):\n",
    "    # TODO: HAND-CODED, NEED TO BE REFINED!!\n",
    "    # TODO: need to low-case to compare?\n",
    "\n",
    "    result = predict(question)[0]\n",
    "    q_type=''\n",
    "    if result=='TIME':\n",
    "        q_type=='NUMBER'\n",
    "    else:\n",
    "        q_type == result\n",
    "#     print q_type\n",
    "    return q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preferred_entity_list(entity_list, sentence_token, question):\n",
    "    preferred_entity_list = []\n",
    "    question_text = word_tokenize(question)\n",
    "    sentence_tag = nltk.pos_tag(sentence_token)\n",
    "    question_tag = nltk.pos_tag(question_text)\n",
    "    \n",
    "    # initialize a list for comparing, and set all elements as 0\n",
    "    is_open_word = [0] * len(sentence_token)\n",
    "    # find an open word in the question\n",
    "    for word, tag in question_tag:\n",
    "        if tag in ['JJ','JJR','JJS','FW','NN','NNS','NNPS','NNP','VBP','VB',\n",
    "                   'VBG','VBN','VBZ','VBP','RB','RBR','RBS']:\n",
    "            # if the open word appears in the sentence, then mark as 1\n",
    "            for i in range(len(sentence_token)):\n",
    "                if sentence_token[i] == word:\n",
    "                    is_open_word[i] = 1\n",
    "                    \n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "    # find the entity which is closest to open-class words\n",
    "    def get_distance(entity):\n",
    "        # get the position of entity, and find the open class words\n",
    "        covered_OCW = 0\n",
    "        position = sentence_token.index(entity)\n",
    "        \n",
    "        # find number of covered open-class words in a given range\n",
    "        #TODO: find the best window parameter\n",
    "        length = len(sentence_token)/5\n",
    "        window_min = position - length\n",
    "        window_max = position + length \n",
    "        # when touch the start or the end of the sentence\n",
    "        if window_min < 0:\n",
    "            window_min = 0\n",
    "            window_max += (0-window_min)\n",
    "        if window_max > (len(sentence_token) - 1):\n",
    "            window_min -= (window_max - len(sentence_token) + 1)\n",
    "            window_max = (len(sentence_token) - 1)\n",
    "        \n",
    "        # get the total number of covered open-class words\n",
    "        for i in range(window_min, window_max + 1):\n",
    "            if is_open_word[i] == 1:            # find an open-class word\n",
    "                covered_OCW += 1\n",
    "        return covered_OCW\n",
    "\n",
    "    # get distance for each entity and choose the best one\n",
    "    all_distance = {}\n",
    "    for entity in entity_list:\n",
    "        try:\n",
    "            all_distance[entity]=get_distance(entity)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return all_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run on development data\n",
    "\n",
    "with open(\"result.txt\",'w') as output_file:\n",
    "#     limit = 0\n",
    "    all_count = 0\n",
    "    correct_sum = 0\n",
    "    corr_sen_retr_count = 0\n",
    "    wrong_but_in_pool = 0\n",
    "    partialy_correct = 0\n",
    "#     no_entity_in_first_sent = 0\n",
    "\n",
    "#     print '='*78\n",
    "#     print 'Use top %d sentences: ' % (limit + 1)\n",
    "    for i in range(len(match_sent)):\n",
    "#     for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "        for j in range(len(match_sent[i])):\n",
    "#             result = ''\n",
    "#             backoff = 0\n",
    "            all_count += 1\n",
    "            cor_answer = dev[i][\"qa\"][j]['answer']\n",
    "            Q = dev[i][\"qa\"][j]['question']\n",
    "            A_sentence = dev[i][\"sentences\"][dev[i][\"qa\"][j]['answer_sentence']]\n",
    "\n",
    "#             result = get_ranked_ans(entity_pool[i][j][0], Q, question_sent_list[i][j][0])\n",
    "            guessed_sentence = dev[i]['sentences'][match_sent[i][j][0]]\n",
    "            guessed_entities = entity_pool[i][j][0]\n",
    "\n",
    "            \n",
    "#             if result == '':\n",
    "#                 no_entity_in_first_sent += 1\n",
    "            \n",
    "#             # If didn't got answer in current sentence, then backoff to next candidate sentence\n",
    "#             # until getting the correct answer\n",
    "            \n",
    "#             while (result == '') and (backoff < limit):\n",
    "#                 backoff += 1\n",
    "#                 result = get_ranked_ans(entity_pool[i][j][backoff], Q, question_sent_list[i][j][backoff])\n",
    "#                 guessed_sentence = dev[i]['sentences'][match_sent[i][j][backoff]]\n",
    "#                 guessed_entities = entity_pool[i][j][backoff]\n",
    "\n",
    "            result1, score1, pool1 = get_ranked_ans(entity_pool[i][j][0], Q, question_sent_list[i][j][0])\n",
    "            result2, score2, pool2 = get_ranked_ans(entity_pool[i][j][1], Q, question_sent_list[i][j][1])\n",
    "            \n",
    "            if score2 * 0.2 > score1:\n",
    "                result = result2\n",
    "                if (result != cor_answer) and (result != '') and (result in pool2):\n",
    "                    wrong_but_in_pool += 1\n",
    "                if (result != cor_answer) and (result in cor_answer):\n",
    "                    partialy_correct += 1\n",
    "            else:\n",
    "                result = result1\n",
    "                if (result != cor_answer) and (result != '') and (result in pool1):\n",
    "                    wrong_but_in_pool += 1\n",
    "                if (result != cor_answer) and (result in cor_answer):\n",
    "                    partialy_correct += 1\n",
    "#             output_file.write('Retrieved Entities: ' + '\\t' + str(guessed_entities) + '\\n\\n')\n",
    "\n",
    "            if result == cor_answer:\n",
    "                correct_sum += 1\n",
    "                if A_sentence == guessed_sentence:\n",
    "                    corr_sen_retr_count += 1\n",
    "            else:\n",
    "                guess_print = 'guessed sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                wrong_1 = '======== WRONG SENTENCES! ======== \\n' + 'guessed sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                wrong_2 = 'CORRECT SENTENCE: ' + '\\t' + A_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                Q_print = 'Q: ' + '\\t' + Q.encode('utf-8') + '\\n\\n'\n",
    "                corr_ans_print = 'CORRECT ANSWER: ' + '\\t' + cor_answer.encode('utf-8') + '\\n'\n",
    "                guessed_ans_print = 'guessed answer: ' + '\\t' + result.encode('utf-8')+\"\\n\"\n",
    "\n",
    "                if A_sentence != guessed_sentence:\n",
    "                    output_file.write(wrong_1)\n",
    "                    output_file.write(wrong_2)\n",
    "                else:\n",
    "                    corr_sen_retr_count += 1\n",
    "                    output_file.write(guess_print)\n",
    "                output_file.write(Q_print)\n",
    "                output_file.write('='*60 + '\\n')\n",
    "                output_file.write(corr_ans_print)\n",
    "                output_file.write(guessed_ans_print)\n",
    "                output_file.write('='*60 + '\\n\\n')\n",
    "\n",
    "    print 'correct sum: ' + str(correct_sum)\n",
    "    print 'Sentence Recall: ' + str((corr_sen_retr_count + 0.0) / all_count)\n",
    "#     print \"%d No.1 ranked sentences has no entity extracted at all.\" % no_entity_in_first_sent\n",
    "#     print 'Wrong but in pool: ', wrong_but_in_pool / all_count\n",
    "    print \"Partialy correct: \", partialy_correct / all_count\n",
    "    print \"【 SCORE : \" + str((correct_sum + 0.0) / all_count) + ' 】\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # run on test data\n",
    "\n",
    "# with open(\"result_to_kaggle.txt\",'w') as output_file:\n",
    "# #     limit = 3\n",
    "#     output_file.write('id,answer'+'\\n')\n",
    "#     for i in range(len(match_sent)):\n",
    "#         for j in range(len(match_sent[i])):\n",
    "#             result = ''\n",
    "#             Q = dev[i][\"qa\"][j]['question']\n",
    "\n",
    "#             result1,score1 = get_ranked_ans(entity_pool[i][j][0], Q, question_sent_list[i][j][0])\n",
    "#             result2,score2 = get_ranked_ans(entity_pool[i][j][1], Q, question_sent_list[i][j][1])\n",
    "            \n",
    "#             if score2 * 0.2>score1:\n",
    "#                 result=result2\n",
    "#             else:\n",
    "#                 result=result1\n",
    "                \n",
    "#             result = result.encode('utf-8')\n",
    "#             reuslt = result.replace('\" ','')\n",
    "#             result = result.replace('\"','')\n",
    "#             result = result.replace(\",\",\"-COMMA-\")\n",
    "#             q_id = dev[i][\"qa\"][j]['id']\n",
    "#             output_file.write(str(q_id) + ',' + str(result) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
