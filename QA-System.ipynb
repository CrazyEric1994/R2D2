{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import nltk\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_file = open(\"data/train.json\",'r')\n",
    "# dev  ----> test\n",
    "dev_file = open(\"data/dev.json\",'r')\n",
    "test_file=open(\"data/test.json\",'r')\n",
    "train = json.loads(train_file.read())\n",
    "dev = json.loads(dev_file.read())\n",
    "test = json.loads(test_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# may be stem?\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "stopword =  stopwords.words()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def doc_word_dict(doc):\n",
    "    word_dict = set()\n",
    "    for sent in doc['sentences']:\n",
    "        for word in  word_tokenizer.tokenize(sent):\n",
    "            word = lemmatize(word.lower())\n",
    "            if word not in stopword:\n",
    "                word_dict.add(word)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_BOW(sent):\n",
    "    term_dict={}\n",
    "    for word in word_tokenizer.tokenize(sent):\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopword:\n",
    "#             term_dict[word]=term_dict.get(word,0)+1\n",
    "            term_dict[word]=1 # 放弃词频\n",
    "    return term_dict\n",
    "\n",
    "def cal_BOW(doc):\n",
    "    doc_term_matrix = [] \n",
    "    for sent in doc['sentences']:\n",
    "        temp = get_BOW(sent)\n",
    "        doc_term_matrix.append(temp)\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_doc_num2(query):\n",
    "    query =  transformer.transform(vectorizer.transform(get_BOW(query)))\n",
    "    result={}\n",
    "    for x in range(term_matrix.shape[0]):\n",
    "         result[x]=cos_distance(query.toarray(),term_matrix[x].toarray())\n",
    "            \n",
    "    minvalue=1\n",
    "    first=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            first=item     \n",
    "    del result[first]\n",
    "    \n",
    "    minvalue=1\n",
    "    second=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            second=item     \n",
    "    return first,second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sentences from documents: 100%|██████████| 40/40 [14:45<00:00, 16.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm='l2')\n",
    "\n",
    "# store guessed question sentence no\n",
    "match_sent= [] #[[(best_match_sent_no,second_match_sent_no),...][second doc]]\n",
    "count = 0\n",
    "\n",
    "for dev_doc in tqdm(dev, desc='Extracting sentences from documents'):\n",
    "    count += 1\n",
    "    doc_match_sent = []\n",
    "    term_matrix = transformer.fit_transform(vectorizer.fit_transform(cal_BOW(dev_doc)))\n",
    "    for qa in dev_doc['qa']:\n",
    "        doc_match_sent.append(get_best_doc_num2(qa['question']))\n",
    "    match_sent.append(doc_match_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of set(mentioned_sent_number in the guessed )\n",
    "mentioned_sent = []\n",
    "\n",
    "for doc in match_sent:\n",
    "    tmp_doc = set()\n",
    "    for first,second in doc:\n",
    "        tmp_doc.add(first)\n",
    "        tmp_doc.add(second)\n",
    "    mentioned_sent.append(tmp_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "# st = StanfordNERTagger('/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "#                '/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/stanford-ner.jar') \n",
    "st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "               '/usr/share/stanford-ner/stanford-ner.jar') \n",
    "\n",
    "# tokenize sentence\n",
    "ner_tag_1 = []\n",
    "for i in range(len(dev)):\n",
    "    doc_tag = []\n",
    "    for j in range(len(dev[i]['sentences'])):\n",
    "        if j in mentioned_sent[i]:# mentioned in the guess\n",
    "            sentence = dev[i]['sentences'][j]\n",
    "            if '\"' in sentence:\n",
    "                sentence = sentence.replace('\"',\"\")\n",
    "            if sentence[:-1]==\".\":\n",
    "                sentence = sentence[:-1]\n",
    "            doc_tag.append(word_tokenizer.tokenize(sentence))\n",
    "        else:\n",
    "            doc_tag.append([])\n",
    "            \n",
    "    doc_tag = st.tag_sents(doc_tag)\n",
    "    ner_tag_1.append(doc_tag)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_sent = \"Satellite test broadcasts started in 1989, with regular testing starting in 1991 and regular broadcasting of BS-9ch commencing on November 25, 1994, which featured commercial and NHK programming.\"\n",
    "\n",
    "st.tag_sents([word_tokenizer.tokenize(test_sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbol_list = [\"(\",\")\",\"[\",\"]\"]\n",
    "\n",
    "def hasSymbol(inputString):\n",
    "    for char in inputString:\n",
    "        if char in symbol_list:\n",
    "#             print char\n",
    "            return False\n",
    "        if not (char.isalpha() or char.isdigit()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    if any(char.isdigit() for char in inputString):\n",
    "        return True\n",
    "    else:\n",
    "        for word in word_tokenizer.tokenize(inputString):\n",
    "            if word in ['one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
    "                  'eight', 'nine', 'eleven', 'twelve', 'thirteen',\n",
    "                  'fourteen', 'fifteen', 'sixteen',\n",
    "                  'ten', 'twenty', 'thirty', 'forty', 'fifty', 'sixty',\n",
    "                  'seventy', 'eighty', 'ninety', 'seventeen', 'eighteen',\n",
    "                  'nineteen']:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_same_word(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-2:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                term_n2,tag_n2 = tag_list[k][i][j+2]\n",
    "                tmp = term+term_n+term_n2\n",
    "                \n",
    "                if hasSymbol(tmp) and tmp in dev[k][\"sentences\"][i]:\n",
    "                    if tag_n2 != 'O':\n",
    "                        temp =  (tmp,tag_n2)\n",
    "                    elif tag_n != 'O':\n",
    "                        temp =  (tmp,tag_n)\n",
    "                    elif tag != 'O':\n",
    "                        temp = (tmp,tag)\n",
    "                    else:\n",
    "                        temp = (tmp,\"OTHER\")\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+2]\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 1\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_entity(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-1:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                tmp = term+term_n\n",
    "                \n",
    "                if tag == tag_n and tag != \"O\" and term_n!=\",\" and term_n !=\";\":\n",
    "                    if tmp in dev[k][\"sentences\"][i]:\n",
    "                        temp =  (tmp,tag)\n",
    "                    else:\n",
    "                        temp =  (term + \" \" + term_n,tag)\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 1\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn O and ORGANIZATION into OTHER; digit into NUMBER\n",
    "def tune_other_and_number(tag_list):\n",
    "    for i in range(len(tag_list)): # each document\n",
    "        for j in range(len(tag_list[i])): # each sentence\n",
    "            for k in range(len(tag_list[i][j])): # each question\n",
    "                term,tag = tag_list[i][j][k] \n",
    "                if term!='' and (tag == \"ORGANIZATION\"  or (len(term)>0 and (term,tag)!=tag_list[i][j][0] and tag == 'O' and term[0].isupper())):\n",
    "                    tag_list[i][j][k] = (term,\"OTHER\")\n",
    "                if  hasNumbers(term):\n",
    "                    tag_list[i][j][k] = (term,\"NUMBER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "ner_tag = []\n",
    "ner_tag = copy.deepcopy(ner_tag_1)\n",
    "\n",
    "combine_same_word(ner_tag)\n",
    "combine_entity(ner_tag)\n",
    "tune_other_and_number(ner_tag)\n",
    "combine_entity(ner_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tags = [\"PERSON\",\"LOCATION\",\"NUMBER\",\"OTHER\",\"ORGANIZATION\",\"PERCENT\"]\n",
    "tags = [\"PERSON\",\"LOCATION\",\"NUMBER\",\"OTHER\"]\n",
    "\n",
    "\n",
    "def create_entity(first,second):\n",
    "    sent_tag_dict = dict.fromkeys(tags,[])\n",
    "    for k in [first]:\n",
    "        for j in ner_tag[i][k]:\n",
    "            term,tag = j\n",
    "            if term =='':\n",
    "                continue\n",
    "#             if tag ==  \"DATE\":\n",
    "#                 regex = re.compile(r'[0-9]{4}')\n",
    "#                 year = regex.findall(term)\n",
    "#                 if year != []:\n",
    "#                     sent_tag_dict[\"YEAR\"] = sent_tag_dict[\"YEAR\"]+ [year]\n",
    "            if tag in tags:\n",
    "                sent_tag_dict[tag] = sent_tag_dict[tag]+ [term]\n",
    "            elif tag == \"DATE\" or tag == \"TIME\" or tag == \"PERCENT\" or hasNumbers(term):\n",
    "                sent_tag_dict[\"NUMBER\"] = sent_tag_dict[\"NUMBER\"]+ [term]\n",
    "    for tag in tags:\n",
    "        sent_tag_dict[tag] = list(set(sent_tag_dict[tag]))\n",
    "    return sent_tag_dict\n",
    "\n",
    "def remove_tag(list_tag):\n",
    "    list_tmp = []\n",
    "    for tup in list_tag:\n",
    "        term,tag = tup\n",
    "        if term != '':\n",
    "            list_tmp.append(term)\n",
    "    return list_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_sent_list = []\n",
    "entity_pool = []\n",
    "\n",
    "for i in range(len(match_sent)):\n",
    "    doc = dev[i]\n",
    "    sent_pool = doc['sentences']\n",
    "    test_list_tmp = []\n",
    "    entity_doc_pool = []\n",
    "    \n",
    "    for first,second in match_sent[i]:\n",
    "        tmp =  ner_tag[i][first]\n",
    "        test_list_tmp.append(remove_tag(tmp))\n",
    "        entity_doc_pool.append(create_entity(first,second))\n",
    "        \n",
    "    question_sent_list.append(test_list_tmp)\n",
    "    entity_pool.append(entity_doc_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_ranked_ans(entities_dic, question, sentence_token):\n",
    "    # identify if the entity set is empty. If True, return nothing\n",
    "    tmp_rank = {}\n",
    "    is_empty = True\n",
    "    for values in entities_dic.values():\n",
    "        if len(values) != 0:\n",
    "            is_empty = False\n",
    "            \n",
    "    if is_empty == False:\n",
    "        q_type = get_question_type(question)\n",
    "        # count the number of 'OTHER' type for analysis\n",
    "        if q_type == 'OTHER':\n",
    "            global OTHER_count \n",
    "            OTHER_count += 1\n",
    "        for ent_type,entities in entities_dic.items():\n",
    "            # answers whose content words all appear in the question should be ranked lowest.\n",
    "            for entity in entities:\n",
    "                tmp_rank[entity] = tmp_rank.setdefault(entity,0)\n",
    "                \n",
    "                #TODO: this should be removed later to be handled in second section\n",
    "                if ('%' in entity) and ('percentage' in question):\n",
    "                    tmp_rank[entity] += 50\n",
    "                if bool(re.match('\\d{4}', entity)) and ('hat year' in question):\n",
    "                    tmp_rank[entity] += 50\n",
    "                    \n",
    "                if (entity.lower() in question.lower()) or (entity.lower().replace('-', ' ') in question.lower()):\n",
    "                    tmp_rank[entity] -= 999\n",
    "            # Answers which match the question type should be ranked higher than those that don't\n",
    "            if ent_type == q_type and ent_type != 'OTHER':\n",
    "                for entity in entities:\n",
    "                    tmp_rank[entity] += 1\n",
    "                ######## TODO: Apply this to all types?\n",
    "            # entity closer in the sentence to a closed-class word should be preferred\n",
    "            preferred_entity = get_preferred_entity(entities, sentence_token, question)\n",
    "            if preferred_entity != None:\n",
    "                tmp_rank[preferred_entity] += 1\n",
    "        # sort and choose the best answer\n",
    "        sorted_ans = sorted(tmp_rank.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        # log for error analysis\n",
    "        output_file.write('Q_type: ' + '\\t' + q_type + '\\n')\n",
    "        output_file.write('Ranked Answers: ' + '\\t' + str(sorted_ans).encode('utf-8') + '\\n\\n')\n",
    "        \n",
    "        # TODO: bug here. list out of index??? why?\n",
    "        if len(sorted_ans) != 0:\n",
    "            best_ans = sorted_ans[0][0]\n",
    "        else:\n",
    "            best_ans = ''\n",
    "        return best_ans\n",
    "       \n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple rule-based question type classifier based on key words \n",
    "\n",
    "def get_question_type(question):\n",
    "    # TODO: HAND-CODED, NEED TO BE REFINED!!\n",
    "    # TODO: need to low-case to compare?\n",
    "\n",
    "    type_rules = [\n",
    "        ('PERSON', [\"Who\", \"who\", \"Whose\", \"whose\", \"Whom\", \"whom\"]),\n",
    "        ('LOCATION', [\"Where\", \"where\", \"area\", \"city\", \"province\", \"located\",\n",
    "                     \"location\"]),\n",
    "        ('NUMBER', [\"When\",\"when\", \"few\", \"little\", \"much\", \"many\", \"size\",\n",
    "                   \"young\", \"old\", \"long\", \"year\", \"years\", \"day\", \"era\",\n",
    "                   \"early\", \"century\", \"population\", \"cost\", \"How far\", \n",
    "                    \"how far\", \"sizes\", \"time\", \"month\", \"century\", \"percentage\"])\n",
    "    ]\n",
    "\n",
    "    q_type = None\n",
    "    for question_type, key_words in type_rules:\n",
    "        if q_type == None:\n",
    "            for key_word in key_words:\n",
    "                if key_word in question:\n",
    "                    q_type = question_type\n",
    "                    break\n",
    "    if q_type == None:\n",
    "        q_type = 'OTHER'\n",
    "\n",
    "    return q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Digital', u'NOUN'), ('cameras', u'NOUN'), ('often', u'ADV'), ('use', u'VERB'), ('infrared', u'ADJ'), ('blockers', u'NOUN'), ('.', u'.')]\n",
      "\n",
      "[('Digital', 'NNP'), ('cameras', 'NNS'), ('often', 'RB'), ('use', 'VBP'), ('infrared', 'JJ'), ('blockers', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "test_sent = \"Digital cameras often use infrared blockers.\"\n",
    "print  nltk.pos_tag(word_tokenizer.tokenize(test_sent),tagset='universal')\n",
    "print\n",
    "print  nltk.pos_tag(word_tokenizer.tokenize(test_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# among entities of the same type, the prefered entity should be \n",
    "# the one which is closer in the sentence to a open-class word\n",
    "# from the question.\n",
    "# ----> nouns, verbs, adjectives, and adverbs.\n",
    "\n",
    "import sys\n",
    "\n",
    "def get_preferred_entity(entity_list, sentence_token, question):\n",
    "    preferred_entity = None\n",
    "    question_text = word_tokenize(question)\n",
    "    sentence_tag = nltk.pos_tag(sentence_token,tagset='universal')\n",
    "    question_tag = nltk.pos_tag(question_text,tagset='universal')\n",
    "    \n",
    "    # initialize a list for comparing, and set all elements as 0\n",
    "    is_open_word = [0] * len(sentence_token)\n",
    "    # find an open word in the question\n",
    "    for word, tag in question_tag:\n",
    "        if tag in ['ADJ','NOUN','VERB','ADV']:\n",
    "            # if the open word appears in the sentence, then mark as 1\n",
    "            for i in range(len(sentence_token)):\n",
    "                if sentence_token[i] == word:\n",
    "                    is_open_word[i] = 1\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "    # find the closest distance to open-class words for an entity\n",
    "    def get_distance(entity):\n",
    "        # get the position of entity, and find the open class words\n",
    "        distances_to_OCW = 0\n",
    "        position = sentence_token.index(entity)\n",
    "\n",
    "        # find distances to all open-class words\n",
    "        for i in range(len(sentence_token)):\n",
    "            if is_open_word[i] == 1:            # find an open-class word\n",
    "                distances_to_OCW += abs(position - i)\n",
    "        if distances_to_OCW == 0:             # didn't find open-class words\n",
    "            distances_to_OCW = sys.maxint       \n",
    "        return distances_to_OCW\n",
    "    \n",
    "    # get distance for each entity and choose the best one\n",
    "    all_distance = []\n",
    "    for entity in entity_list:\n",
    "        all_distance.append(get_distance(entity))\n",
    "        preferred_entity = entity_list[all_distance.index(min(all_distance))]\n",
    "\n",
    "    return preferred_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import sys\n",
    "\n",
    "# def get_preferred_entity_2(entity_list, sentence_token, question):\n",
    "#     preferred_entity = None\n",
    "#     question_text = word_tokenize(question)\n",
    "#     sentence_tag = nltk.pos_tag(sentence_token,tagset='universal')\n",
    "#     question_tag = nltk.pos_tag(question_text,tagset='universal')\n",
    "    \n",
    "#     # initialize a list for comparing, and set all elements as 0\n",
    "#     is_open_word = [0] * len(sentence_token)\n",
    "#     # find an open word in the question\n",
    "#     for word, tag in question_tag:\n",
    "#         if tag in ['ADJ','NOUN','VERB','ADV']:\n",
    "#             # if the open word appears in the sentence, then mark as 1\n",
    "#             for i in range(len(sentence_token)):\n",
    "#                 if sentence_token[i] == word:\n",
    "#                     is_open_word[i] = 1\n",
    "                    \n",
    "# ##############################################################################\n",
    "\n",
    "#     # find the entity which is closest to open-class words\n",
    "#     def get_distance(entity):\n",
    "#         # get the position of entity, and find the open class words\n",
    "#         covered_OCW = 0\n",
    "#         position = sentence_token.index(entity)\n",
    "        \n",
    "#         # find number of covered open-class words in a given range\n",
    "#         #TODO: find the best window parameter\n",
    "#         length = len(sentence_token)/3\n",
    "#         window_min = position - length\n",
    "#         window_max = position + length\n",
    "#         # when touch the start or the end of the sentence\n",
    "#         if window_min < 0:\n",
    "#             window_min = 0\n",
    "#             window_max += (0-window_min)\n",
    "#         if window_max > (len(sentence_token) - 1):\n",
    "#             window_min -= (window_max - len(sentence_token) + 1)\n",
    "#             window_max = (len(sentence_token) - 1)\n",
    "        \n",
    "#         # get the total number of covered open-class words\n",
    "#         for i in range(window_min, window_max + 1):\n",
    "#             if is_open_word[i] == 1:            # find an open-class word\n",
    "#                 covered_OCW += 1\n",
    "                \n",
    "#         if covered_OCW == 0:             # didn't find open-class words\n",
    "#             covered_OCW = -(sys.maxint)\n",
    "        \n",
    "#         return covered_OCW\n",
    "    \n",
    "#     # get distance for each entity and choose the best one\n",
    "#     all_distance = []\n",
    "#     for entity in entity_list:\n",
    "#         all_distance.append(get_distance(entity))\n",
    "#         all_best_index = [i for i, x in enumerate(li) if x == max(li)]\n",
    "        \n",
    "#         ??? preferred_entity = entity_list[all_distance.index()]\n",
    "    \n",
    "    \n",
    "#     return preferred_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering:  68%|██████▊   | 27/40 [01:15<00:37,  2.89s/it]"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "count = 1\n",
    "correct_sum = 0\n",
    "corr_sen_retr_count = 0\n",
    "OTHER_count = 0\n",
    "\n",
    "with open(\"result.txt\",'w') as output_file:\n",
    "    for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "        for j in range(len(match_sent[i])):\n",
    "            result = get_ranked_ans(entity_pool[i][j], dev[i][\"qa\"][j]['question'], question_sent_list[i][j])\n",
    "            output_file.write('Retrieved Entities: ' + '\\t' + str(entity_pool[i][j]) + '\\n\\n')\n",
    "            count += 1\n",
    "            cor_answer = dev[i][\"qa\"][j]['answer']\n",
    "            Q = dev[i][\"qa\"][j]['question']\n",
    "            A_sentence = dev[i][\"sentences\"][dev[i][\"qa\"][j]['answer_sentence']]\n",
    "            sent_1, sent_2 = match_sent[i][j]\n",
    "            guessed_sentence = dev[i]['sentences'][sent_1]         # + ' ' + dev[i]['sentences'][sent_2]\n",
    "            \n",
    "            if result == cor_answer:\n",
    "                correct_sum += 1\n",
    "            else:\n",
    "                string1 = 'Retrieved Sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string1_1 = '==== WRONG SENTENCES! ==== \\n' + 'Guessed_Sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string1_2 = 'CORRECT_Sentence: ' + '\\t' + A_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string2 = 'Q: ' + '\\t' + Q.encode('utf-8') + '\\n\\n'\n",
    "                string3 = 'CORRECT_ANSWER: ' + '\\t' + cor_answer.encode('utf-8') + '\\n'\n",
    "                string4 = 'GUESSED_ANSWER: ' + '\\t' + result.encode('utf-8')+\"\\n\"\n",
    "                \n",
    "            if A_sentence not in guessed_sentence:\n",
    "                output_file.write(string1_1)\n",
    "                output_file.write(string1_2)\n",
    "            else:\n",
    "                corr_sen_retr_count += 1\n",
    "                output_file.write(string1)\n",
    "            output_file.write(string2)\n",
    "            output_file.write('='*60 + '\\n')\n",
    "            output_file.write(string3)\n",
    "            output_file.write(string4)\n",
    "            output_file.write('='*60 + '\\n\\n')\n",
    "    print 'correct sum: ' + str(correct_sum)\n",
    "    print 'Sentence Recall: ' + str((corr_sen_retr_count+0.0)/count)\n",
    "    print \"'OTHER': \" + str(OTHER_count)\n",
    "    \n",
    "for i in dev:\n",
    "    for j in i[\"qa\"]:\n",
    "        num += 1\n",
    "print (correct_sum+0.0)/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # run on test data\n",
    "\n",
    "# with open(\"result_to_kaggle.txt\",'w') as output_file:\n",
    "#     output_file.write('id,answer'+'\\n')\n",
    "#     for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "#         for j in range(len(match_sent[i])):\n",
    "#             result = get_ranked_ans(entity_pool[i][j], dev[i][\"qa\"][j]['question'], question_sent_list[i][j])\n",
    "#             result = result.encode('utf-8')\n",
    "#             reuslt = result.replace('\" ','')\n",
    "#             result = result.replace('\"','')\n",
    "#             result = result.replace(\",\",\"-COMMA-\")\n",
    "#             q_id = dev[i][\"qa\"][j]['id']\n",
    "#             output_file.write(str(q_id) + ',' + str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demo_entity_list = [u'pre-set']\n",
    "# demo_sentence = 'For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, \\\n",
    "#                     and those with a lower emissivity will appear cooler.'\n",
    "# demo_sentence_token = demo_sentence.split()\n",
    "# question = 'How will the infrared image of an object with a higher emissivity appear in relation to one with lower emissivity?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
