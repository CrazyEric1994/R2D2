{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import nltk\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_file = open(\"data/train.json\",'r')\n",
    "\n",
    "# dev  ----> test\n",
    "dev_file = open(\"data/dev.json\",'r')\n",
    "test_file=open(\"data/test.json\",'r')\n",
    "train = json.loads(train_file.read())\n",
    "# dev = json.loads(dev_file.read())\n",
    "dev = json.loads(test_file.read())\n",
    "# test = json.loads(test_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# may be stem?\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "stopword =  stopwords.words()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def doc_word_dict(doc):\n",
    "    word_dict = set()\n",
    "    for sent in doc['sentences']:\n",
    "        for word in  word_tokenizer.tokenize(sent):\n",
    "            word = lemmatize(word.lower())\n",
    "            if word not in stopword:\n",
    "                word_dict.add(word)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_BOW(sent):\n",
    "    term_dict={}\n",
    "    for word in word_tokenizer.tokenize(sent):\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopword:\n",
    "            term_dict[word]=term_dict.setdefault(word,0)+1\n",
    "    return term_dict\n",
    "\n",
    "def cal_BOW(doc):\n",
    "    doc_term_matrix = [] \n",
    "    for sent in doc['sentences']:\n",
    "        temp = get_BOW(sent)\n",
    "        doc_term_matrix.append(temp)\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_doc_num2(query):\n",
    "    query =  transformer.transform(vectorizer.transform(get_BOW(query)))\n",
    "    result={}\n",
    "    for x in range(term_matrix.shape[0]):\n",
    "         result[x]=cos_distance(query.toarray(),term_matrix[x].toarray())\n",
    "            \n",
    "    minvalue=1\n",
    "    first=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            first=item     \n",
    "    del result[first]\n",
    "    \n",
    "    minvalue=1\n",
    "    second=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            second=item     \n",
    "    return first,second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sentences from documents: 100%|██████████| 42/42 [13:38<00:00, 21.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "# store guessed question sentence no\n",
    "match_sent= [] #[[(best_match_sent_no,second_match_sent_no),...][second doc]]\n",
    "count = 0\n",
    "\n",
    "for dev_doc in tqdm(dev, desc='Extracting sentences from documents'):\n",
    "    count += 1\n",
    "    doc_match_sent = []\n",
    "    term_matrix = transformer.fit_transform(vectorizer.fit_transform(cal_BOW(dev_doc)))\n",
    "    for qa in dev_doc['qa']:\n",
    "        doc_match_sent.append(get_best_doc_num2(qa['question']))\n",
    "    match_sent.append(doc_match_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of set(mentioned_sent_number in the guessed )\n",
    "mentioned_sent = []\n",
    "\n",
    "for doc in match_sent:\n",
    "    tmp_doc = set()\n",
    "    for first,second in doc:\n",
    "        tmp_doc.add(first)\n",
    "        tmp_doc.add(second)\n",
    "    mentioned_sent.append(tmp_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sent(sentence):\n",
    "    sentence = sentence.split(\".\")[0]\n",
    "    tmp = []\n",
    "    tmp += sentence.split(\",\")\n",
    "    sentence  = tmp[0].split()\n",
    "    for fraction  in tmp[1:]:\n",
    "        sentence.append(\",\")\n",
    "        sentence.extend(fraction.split())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "               '/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/stanford-ner.jar') \n",
    "# st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "#                '/usr/share/stanford-ner/stanford-ner.jar') \n",
    "\n",
    "# tokenize sentence\n",
    "test_tag = []\n",
    "for i in range(len(dev)):\n",
    "    test_sent_tag = []\n",
    "    for j in range(len(dev[i]['sentences'])):\n",
    "        if j in mentioned_sent[i]:# mentioned in the guess\n",
    "            test_sent_tag.append(word_tokenizer.tokenize(dev[i]['sentences'][j]))\n",
    "#             test_sent_tag.append(split_sent(dev[i]['sentences'][j]))\n",
    "        else:\n",
    "            test_sent_tag.append([])\n",
    "    test_sent_tag = st.tag_sents(test_sent_tag)\n",
    "    test_tag.append(test_sent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn O and ORGANIZATION into other\n",
    "def tune_other(tag_list):\n",
    "    for i in range(len(tag_list)): # each document\n",
    "        for j in range(len(tag_list[i])): # each sentence\n",
    "            for k in range(len(tag_list[i][j])): # each question\n",
    "                term,tag = tag_list[i][j][k] \n",
    "                if term!='' and (tag == \"ORGANIZATION\"  or (len(term)>0 and (term,tag)!=tag_list[i][j][0] and tag == 'O' and term[0].isupper())):\n",
    "                    tag_list[i][j][k] = (term,\"OTHER\")\n",
    "\n",
    "tune_other(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZhangJiaWei/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/ipykernel_launcher.py:16: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def combine_entity(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-2:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                if tag == tag_n and tag != \"O\" :\n",
    "                    if term_n != \",\" and  term_n !=\"%\":\n",
    "                        temp =  (term + \" \" + term_n,tag)\n",
    "                    else:\n",
    "                        temp =  (term + term_n,tag)\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 1\n",
    "                if term == \"-\" or term.encode('utf8') == \"\\xe2\\x80\\x93\" or term == \"\\x80\\x93\":\n",
    "                    temp =  (tag_list[k][i][j-1][0] + term + term_n,tag_list[k][i][j-1][1])\n",
    "                    tag_list[k][i][j-1] = temp\n",
    "                    del tag_list[k][i][j]\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 2\n",
    "                if term == \",\" and tag_list[k][i][j-1][0].isdigit() and tag_list[k][i][j+1][0].isdigit():\n",
    "                    temp =  (tag_list[k][i][j-1][0] + term + term_n,tag_list[k][i][j-1][1])\n",
    "                    tag_list[k][i][j-1] = temp\n",
    "                    del tag_list[k][i][j]\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 2\n",
    "                j += 1\n",
    "\n",
    "combine_entity(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn O and ORGANIZATION into other\n",
    "def tune_other_2(tag_list):\n",
    "    for i in range(len(tag_list)): # each document\n",
    "        for j in range(len(tag_list[i])): # each sentence\n",
    "            for k in range(len(tag_list[i][j])): # each question\n",
    "                term,tag = tag_list[i][j][k] \n",
    "                if \"-\" in term and tag ==\"O\":\n",
    "                    tag_list[i][j][k] = (term,\"OTHER\")\n",
    "\n",
    "tune_other_2(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_entity(first,second):\n",
    "    sent_tag_dict = dict.fromkeys(tags,[])\n",
    "    for k in [first,second]:\n",
    "        for j in test_tag[i][k]:\n",
    "            term,tag = j\n",
    "            if term =='':\n",
    "                continue\n",
    "            if tag == \"PERSON\" or tag == \"LOCATION\" or tag == \"OTHER\":\n",
    "                sent_tag_dict[tag] = sent_tag_dict[tag]+ [term]\n",
    "            elif tag == \"DATE\" or tag == \"TIME\" or tag == \"PERCENT\" or hasNumbers(term):\n",
    "                sent_tag_dict[\"NUMBER\"] = sent_tag_dict[\"NUMBER\"]+ [term]\n",
    "    return sent_tag_dict\n",
    "\n",
    "def remove_tag(list_tag):\n",
    "    list_tmp = []\n",
    "    for tup in list_tag:\n",
    "        term,tag = tup\n",
    "        if term != '':\n",
    "            list_tmp.append(term)\n",
    "    return list_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = []\n",
    "entity_pool = []\n",
    "tags = [\"PERSON\",\"LOCATION\",\"NUMBER\",\"OTHER\"]\n",
    "\n",
    "for i in range(len(match_sent)):\n",
    "    doc = dev[i]\n",
    "    sent_pool = doc['sentences']\n",
    "    test_list_tmp = []\n",
    "    entity_doc_pool = []\n",
    "    \n",
    "    for first,second in match_sent[i]:\n",
    "        tmp =  test_tag[i][first]+test_tag[i][second]\n",
    "        test_list_tmp.append(remove_tag(tmp))\n",
    "        entity_doc_pool.append(create_entity(first,second))\n",
    "        \n",
    "    test_list.append(test_list_tmp)\n",
    "    entity_pool.append(entity_doc_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import operator\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_ranked_ans(entities_dic, question, sentence):\n",
    "    # identify if the entity set is empty. If True, return nothing\n",
    "    is_empty = True\n",
    "    for values in entities_dic.values():\n",
    "        if len(values) != 0:\n",
    "            is_empty = False\n",
    "            \n",
    "    if is_empty == False:\n",
    "        q_type = get_question_type(question)\n",
    "        tmp_rank = {}\n",
    "        for ent_type,entities in entities_dic.items():\n",
    "            # answers whose content words all appear in the question should be ranked lowest.\n",
    "            for entity in entities:\n",
    "                tmp_rank[entity] = tmp_rank.setdefault(entity,0)\n",
    "                if entity in question:\n",
    "                    tmp_rank[entity] = tmp_rank.setdefault(entity,0) - 1\n",
    "            # Answers which match the question type should be ranked higher than those that don't\n",
    "            if ent_type == q_type and ent_type != 'OTHER':\n",
    "                for entity in entities:\n",
    "                    tmp_rank[entity] = tmp_rank.setdefault(entity,0) + 1\n",
    "                ######## TODO: Apply this to all types?\n",
    "            # entity closer in the sentence to a closed-class word should be preferred\n",
    "            preferred_entity = get_preferred_entity(entities, sentence, question)\n",
    "            if preferred_entity != None:\n",
    "                tmp_rank[preferred_entity] = tmp_rank.setdefault(preferred_entity,0) + 1\n",
    "        # sort and choose the best answer\n",
    "        sorted_ans = sorted(tmp_rank.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        # TODO: bug here. list out of index??? why?\n",
    "        if len(sorted_ans) != 0:\n",
    "            best_ans = sorted_ans[0][0]\n",
    "        else:\n",
    "            best_ans = ''\n",
    "        return best_ans\n",
    "        # log for error analysis\n",
    "        output_file.write('Q_type: ' + '\\t' + q_type + '\\n')\n",
    "        output_file.write('Ranked Answers: ' + '\\t' + str(sorted_ans).encode('utf-8') + '\\n\\n')\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple rule-based question type classifier based on key words \n",
    "\n",
    "def get_question_type(question):\n",
    "    # TODO: HAND-CODED, NEED TO BE REFINED!!\n",
    "    # TODO: need to low-case to compare?\n",
    "    \n",
    "    type_rules = [\n",
    "        ('PERSON', [\"Who\", \"Whose\", \"Whom\", \"whom\"]),\n",
    "        ('LOCATION', [\"Where\"]),\n",
    "        ('NUMBER', [\"When\", \"few\", \"little\", \"much\", \"many\",\"size\",\n",
    "                   \"young\", \"old\", \"long\", \"year\", \"years\", \"day\"])\n",
    "    ]\n",
    "    \n",
    "    q_type = None\n",
    "    for question_type, key_words in type_rules:\n",
    "        if q_type == None:\n",
    "            for key_word in key_words:\n",
    "                if key_word in question:\n",
    "                    q_type = question_type\n",
    "                    break\n",
    "    if q_type == None:\n",
    "        q_type = 'OTHER'\n",
    "\n",
    "    return q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# among entities of the same type, the prefered entity should be \n",
    "# the one which is closer in the sentence to a open-class word\n",
    "# from the question.\n",
    "# ----> nouns, verbs, adjectives, and adverbs.\n",
    "\n",
    "def get_preferred_entity(entity_list, sentence, question):\n",
    "    preferred_entity = None\n",
    "    sentence_text = sentence\n",
    "    question_text = split_sent(question)\n",
    "    sentence_tag = nltk.pos_tag(sentence_text,tagset='universal')\n",
    "    question_tag = nltk.pos_tag(question_text,tagset='universal')\n",
    "    \n",
    "    # initialize a list for comparing, and set all elements as 0\n",
    "    is_open_word = [0] * len(sentence_text)\n",
    "    # find an open word in the question\n",
    "    for word, tag in question_tag:\n",
    "        if tag in ['ADJ','NOUN','VERB','ADV']:\n",
    "            # if the open word appears in the sentence, then mark as 1\n",
    "            for i in range(len(sentence_text)):\n",
    "                if sentence_text[i] == word:\n",
    "                    is_open_word[i] = 1\n",
    "    \n",
    "    # find the closest distance to an open-class word for an entity\n",
    "    def get_distance(entity):\n",
    "        # get the position of entity, and find the open class word \n",
    "        # from the nearest at both sides\n",
    "        distance = None\n",
    "        position = sentence_text.index(entity)\n",
    "        for i in range(1, len(sentence_text)):\n",
    "            if position - i >= 0:\n",
    "                if is_open_word[position - i] == 1:  # find an open-class word on the left\n",
    "                    distance = i\n",
    "                    break\n",
    "                elif position + i < len(is_open_word):  # find an open-class word on the right\n",
    "                    if is_open_word[position + i] == 1:\n",
    "                        distance = i\n",
    "                        break\n",
    "                else:\n",
    "                    distance = len(sentence_text) + 1  # didn't find open-class words\n",
    "        return distance\n",
    "    \n",
    "    # get distance for each entity and choose the best one\n",
    "    all_distance = []\n",
    "    for entity in entity_list:\n",
    "        all_distance.append(get_distance(entity))\n",
    "        preferred_entity = entity_list[all_distance.index(min(all_distance))]\n",
    "\n",
    "    return preferred_entity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering:   0%|          | 0/42 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-893af2ffa0e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#             q_id = dev[i][\"qa\"][j]['id']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mcor_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mA_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qa\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'answer'"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "count = 1\n",
    "correct_sum = 0\n",
    "corr_sen_retr_count = 0\n",
    "\n",
    "with open(\"result.txt\",'w') as output_file:\n",
    "    for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "        for j in range(len(match_sent[i])):\n",
    "            result = get_ranked_ans(entity_pool[i][j], dev[i][\"qa\"][j]['question'], test_list[i][j])\n",
    "            output_file.write('Retrieved Entities: ' + '\\t' + str(entity_pool[i][j]) + '\\n\\n')\n",
    "#             q_id = dev[i][\"qa\"][j]['id']\n",
    "            count += 1\n",
    "            cor_answer = dev[i][\"qa\"][j]['answer']\n",
    "            Q = dev[i][\"qa\"][j]['question']\n",
    "            A_sentence = dev[i][\"sentences\"][dev[i][\"qa\"][j]['answer_sentence']]\n",
    "            sent_1, sent_2 = match_sent[i][j]\n",
    "            guessed_sentence = dev[i]['sentences'][sent_1] + ' ' + dev[i]['sentences'][sent_2]\n",
    "            \n",
    "            if result == cor_answer:\n",
    "                correct_sum += 1\n",
    "            else:\n",
    "                string1 = 'Retrieved Sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string1_1 = '==== WRONG SENTENCES! ==== \\n' + 'Guessed_Sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string1_2 = 'CORRECT_Sentence: ' + '\\t' + A_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string2 = 'Q: ' + '\\t' + Q.encode('utf-8') + '\\n\\n'\n",
    "                string3 = 'CORRECT_ANSWER: ' + '\\t' + cor_answer.encode('utf-8') + '\\n'\n",
    "                string4 = 'GUESSED_ANSWER: ' + '\\t' + result.encode('utf-8')+\"\\n\"\n",
    "                \n",
    "                if A_sentence not in guessed_sentence:\n",
    "                    output_file.write(string1_1)\n",
    "                    output_file.write(string1_2)\n",
    "                else:\n",
    "                    corr_sen_retr_count += 1\n",
    "                    output_file.write(string1)\n",
    "                output_file.write(string2)\n",
    "                output_file.write('='*60 + '\\n')\n",
    "                output_file.write(string3)\n",
    "                output_file.write(string4)\n",
    "                output_file.write('='*60 + '\\n\\n')\n",
    "    print 'correct sum: ' + str(correct_sum)\n",
    "    print 'Sentence Recall: ' + str((corr_sen_retr_count+0.0)/count)\n",
    "    \n",
    "for i in dev:\n",
    "    for j in i[\"qa\"]:\n",
    "        num += 1\n",
    "print (correct_sum+0.0)/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # run on test data\n",
    "\n",
    "# with open(\"result.txt\",'w') as output_file:\n",
    "#     output_file.write('id,answer'+'\\n')\n",
    "#     for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "#         for j in range(len(match_sent[i])):\n",
    "#             result = get_ranked_ans(entity_pool[i][j], dev[i][\"qa\"][j]['question'], test_list[i][j])\n",
    "#             result = result.encode('utf-8')\n",
    "#             reuslt = result.replace('\" ','')\n",
    "#             result = result.replace('\"','')\n",
    "#             result = result.replace(\",\",\"-COMMA-\")\n",
    "#             q_id = dev[i][\"qa\"][j]['id']\n",
    "#             output_file.write(str(q_id) + ',' + str(result) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
