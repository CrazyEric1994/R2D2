{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import nltk\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_file = open(\"train.json\",'r')\n",
    "dev_file = open(\"dev.json\",'r')\n",
    "test_file=open(\"test.json\",'r')\n",
    "train = json.loads(train_file.read())\n",
    "dev = json.loads(dev_file.read())\n",
    "test = json.loads(test_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# may be stem?\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "stopword =  stopwords.words()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def doc_word_dict(doc):\n",
    "    word_dict = set()\n",
    "    for sent in doc['sentences']:\n",
    "        for word in  word_tokenizer.tokenize(sent):\n",
    "            word = lemmatize(word.lower())\n",
    "            if word not in stopword:\n",
    "                word_dict.add(word)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_BOW(sent):\n",
    "    term_dict={}\n",
    "    for word in word_tokenizer.tokenize(sent):\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopword:\n",
    "            term_dict[word]=term_dict.setdefault(word,0)+1\n",
    "    return term_dict\n",
    "\n",
    "def cal_BOW(doc):\n",
    "    doc_term_matrix = [] \n",
    "    for sent in doc['sentences']:\n",
    "        temp = get_BOW(sent)\n",
    "        doc_term_matrix.append(temp)\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_doc_num2(query):\n",
    "    query =  transformer.transform(vectorizer.transform(get_BOW(query)))\n",
    "    result={}\n",
    "    for x in range(term_matrix.shape[0]):\n",
    "         result[x]=cos_distance(query.toarray(),term_matrix[x].toarray())\n",
    "            \n",
    "    minvalue=1\n",
    "    first=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            first=item     \n",
    "    del result[first]\n",
    "    \n",
    "    minvalue=1\n",
    "    second=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            second=item     \n",
    "    return first,second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sentences from documents: :  28%|██▊       | 11/40 [01:46<02:40,  5.54s/it]/Users/ZhangJiaWei/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "Extracting sentences from documents: : 100%|██████████| 40/40 [11:21<00:00, 15.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "match_sent= []\n",
    "count = 0\n",
    "\n",
    "for dev_doc in tqdm(dev, desc='Extracting sentences from documents'):\n",
    "    count += 1\n",
    "    doc_match_sent = []\n",
    "    term_matrix = transformer.fit_transform(vectorizer.fit_transform(cal_BOW(dev_doc)))\n",
    "    for qa in dev_doc['qa']:\n",
    "        doc_match_sent.append(get_best_doc_num2(qa['question']))\n",
    "    match_sent.append(doc_match_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "               '/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/stanford-ner.jar') \n",
    "\n",
    "test_tag = []\n",
    "for i in range(len(match_sent)):\n",
    "    test_sent_tag = []\n",
    "    for j,k in match_sent[i]:\n",
    "        test_sent_tag.append(word_tokenizer.tokenize(dev[i]['sentences'][j] + ' ' + dev[i]['sentences'][k]))\n",
    "    test_sent_tag = st.tag_sents(test_sent_tag)\n",
    "    test_tag.append(test_sent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_other(tag_list):\n",
    "    for i in range(len(tag_list)):\n",
    "        for j in range(len(tag_list[i])):\n",
    "            for k in range(len(tag_list[i][j])):\n",
    "                term,tag = tag_list[i][j][k]\n",
    "                if term!='' and (tag == \"ORGANIZATION\"  or (len(term)>0 and (term,tag)!=tag_list[i][j][0] and tag == 'O' and term[0].isupper())):\n",
    "                    tag_list[i][j][k] = (term,\"OTHER\")\n",
    "\n",
    "tune_other(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_entity(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-2:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                if tag == tag_n and tag != \"O\":\n",
    "                    temp =  (term + \" \" + term_n,tag)\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+1]\n",
    "                j += 1\n",
    "\n",
    "combine_entity(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = []\n",
    "for k in test_tag:\n",
    "    test_doc_list = []\n",
    "    for i in k:\n",
    "        test_ = []\n",
    "        for term,tag in i:\n",
    "            if term != '':\n",
    "                test_.append(term)\n",
    "        test_doc_list.append(test_)\n",
    "    test_list.append(test_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities: : 100%|██████████| 40/40 [00:01<00:00, 23.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "tags = [\"PERSON\",\"LOCATION\",\"NUMBER\",\"OTHER\"]\n",
    "entity_pool = []\n",
    "for k in tqdm(test_tag, desc='Extracting entities'):\n",
    "    entity_doc_pool = []\n",
    "    for i in k:\n",
    "        sent_tag_dict = dict.fromkeys(tags,[])\n",
    "        for j in range(len(i)):\n",
    "            term,tag = i[j]\n",
    "            if tag == \"PERSON\" or tag == \"LOCATION\" or tag == \"OTHER\":\n",
    "                sent_tag_dict[tag] = sent_tag_dict[tag]+ [term]\n",
    "            elif tag == \"DATE\" or tag == \"TIME\" or tag == \"PERCENT\" or hasNumbers(term):\n",
    "                sent_tag_dict[\"NUMBER\"] = sent_tag_dict[\"NUMBER\"]+ [term]\n",
    "        entity_doc_pool.append(sent_tag_dict)\n",
    "    entity_pool.append(entity_doc_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import nltk\n",
    "\n",
    "# A simple rule-based question type classifier based on key words \n",
    "def get_question_type(question):\n",
    "    # TODO: HAND-CODED, NEED TO BE REFINED!!\n",
    "    # TODO: need to low-case to compare?\n",
    "    \n",
    "    type_rules = {'PERSON':[\"Who\", \"Whose\", \"Whom\"],\n",
    "                  'LOCATION':[\"Where\"],\n",
    "                  'NUMBER':[\"When\", \"few\", \"little\", \"much\", \"many\",\"size\",\n",
    "                            \"young\", \"old\", \"long\", \"year\", \"years\"]\n",
    "                 }\n",
    "    q_type = None\n",
    "    for question_type, key_words in type_rules.items():\n",
    "        for key_word in key_words:\n",
    "            if key_word in question:\n",
    "                q_type = question_type\n",
    "                break\n",
    "        if q_type == None:\n",
    "            q_type = 'OTHER'\n",
    "    return q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# among entities of the same type, the prefered entity should be \n",
    "# the one which is closer in the sentence to a open-class word\n",
    "# from the question.\n",
    "# ----> nouns, verbs, adjectives, and adverbs.\n",
    "def get_preferred_entity(entity_list, sentence, question):\n",
    "    preferred_entity = None\n",
    "    sentence_text = sentence\n",
    "    question_text = word_tokenize(question)\n",
    "    sentence_tag = nltk.pos_tag(sentence_text,tagset='universal')\n",
    "    question_tag = nltk.pos_tag(question_text,tagset='universal')\n",
    "    \n",
    "    # initialize a list for comparing, and set all elements as 0\n",
    "    is_open_word = [0] * len(sentence_text)\n",
    "    # find an open word in the question\n",
    "    for word, tag in question_tag:\n",
    "        if tag in ['ADJ','NOUN','VERB','ADV']:\n",
    "            # if the open word appears in the sentence, then mark as 1\n",
    "            for i in range(len(sentence_text)):\n",
    "                if sentence_text[i] == word:\n",
    "                    is_open_word[i] = 1\n",
    "#     print is_open_word\n",
    "    \n",
    "    # find the closest distance to an open-class word for an entity\n",
    "    def get_distance(entity):\n",
    "        # get the position of entity, and find the open class word \n",
    "        # from the nearest at both sides\n",
    "        distance = None\n",
    "        position = sentence_text.index(entity)\n",
    "        for i in range(1, len(sentence_text)):\n",
    "            if position - i >= 0:\n",
    "                if is_open_word[position - i] == 1:  # find an open-class word on the left\n",
    "                    distance = i\n",
    "                    break\n",
    "                elif position + i < len(is_open_word):  # find an open-class word on the right\n",
    "                    if is_open_word[position + i] == 1:\n",
    "                        distance = i\n",
    "                        break\n",
    "                else:\n",
    "                    distance = len(sentence_text) + 1  # didn't find open-class words\n",
    "        return distance\n",
    "    \n",
    "    # get distance for each entity and choose the best one\n",
    "    all_distance = []\n",
    "    for entity in entity_list:\n",
    "        all_distance.append(get_distance(entity))\n",
    "        preferred_entity = entity_list[all_distance.index(min(all_distance))]\n",
    "\n",
    "    return preferred_entity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ranked_ans(entities_dic, question, sentence):\n",
    "    \n",
    "    # identify if the entity set is empty. If True, return nothing\n",
    "    is_empty = True\n",
    "    for values in entities_dic.values():\n",
    "        if len(values) != 0:\n",
    "            is_empty = False\n",
    "            \n",
    "    if is_empty == False:\n",
    "        q_type = get_question_type(question)\n",
    "        tmp_rank = {}\n",
    "        for ent_type,entities in entities_dic.items():\n",
    "            # answers whose content words all appear in the question should be ranked lowest.\n",
    "            for entity in entities:\n",
    "                if entity in question:\n",
    "                    tmp_rank[entity] = tmp_rank.setdefault(entity,0) - 1\n",
    "            # Answers which match the question type should be ranked higher than those that don't\n",
    "            if ent_type == q_type:\n",
    "                for entity in entities:\n",
    "                    tmp_rank[entity] = tmp_rank.setdefault(entity,0) + 1\n",
    "                ######## TODO: Apply this to all types?\n",
    "                    # entity closer in the sentence to a closed-class word should be preferred\n",
    "                    tmp_rank[get_preferred_entity(entities, sentence, question)] = tmp_rank.setdefault(entity,0) + 1\n",
    "        # sort and choose the best answer\n",
    "        sorted_ans = sorted(tmp_rank.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        #####TODO: bug here. list out of index??? why?\n",
    "        if len(sorted_ans) != 0:\n",
    "            best_ans = sorted_ans[0][0]\n",
    "        else:\n",
    "            best_ans = ''\n",
    "        return best_ans\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Answering: 100%|██████████| 40/40 [02:36<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct sum: 610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "correct_sum = 0\n",
    "with open(\"result.txt\",'w') as output_file:\n",
    "    for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "        for j in range(len(match_sent[i])):\n",
    "            result = get_ranked_ans(entity_pool[i][j], dev[i][\"qa\"][j]['question'], test_list[i][j])\n",
    "#             q_id = dev[i][\"qa\"][j]['id']\n",
    "            count += 1\n",
    "            cor_answer = dev[i][\"qa\"][j]['answer']\n",
    "            if result == cor_answer:\n",
    "                correct_sum += 1\n",
    "            else:\n",
    "                string1 = 'CORRECT_ANSWER: ' + '\\t' + cor_answer.encode('utf-8') + '\\n'\n",
    "                string2 = 'GUESSED_ANSWER: ' + '\\t' + result.encode('utf-8')+\"\\n\\n\"\n",
    "                output_file.write(string1)\n",
    "                output_file.write(string2)\n",
    "    print 'correct sum: ' + str(correct_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0720784591752\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for i in dev:\n",
    "    for j in i[\"qa\"]:\n",
    "        num += 1\n",
    "print (correct_sum+0.0)/num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
