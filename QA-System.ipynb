{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import nltk\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_file = open(\"data/train.json\",'r')\n",
    "\n",
    "# dev  ----> test\n",
    "dev_file = open(\"data/dev.json\",'r')\n",
    "test_file=open(\"data/test.json\",'r')\n",
    "train = json.loads(train_file.read())\n",
    "# dev = json.loads(dev_file.read())\n",
    "dev = json.loads(test_file.read())\n",
    "# test = json.loads(test_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# may be stem?\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "stopword =  stopwords.words()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def doc_word_dict(doc):\n",
    "    word_dict = set()\n",
    "    for sent in doc['sentences']:\n",
    "        for word in  word_tokenizer.tokenize(sent):\n",
    "            word = lemmatize(word.lower())\n",
    "            if word not in stopword:\n",
    "                word_dict.add(word)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_BOW(sent):\n",
    "    term_dict={}\n",
    "    for word in word_tokenizer.tokenize(sent):\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopword:\n",
    "            term_dict[word]=term_dict.setdefault(word,0)+1\n",
    "    return term_dict\n",
    "\n",
    "def cal_BOW(doc):\n",
    "    doc_term_matrix = [] \n",
    "    for sent in doc['sentences']:\n",
    "        temp = get_BOW(sent)\n",
    "        doc_term_matrix.append(temp)\n",
    "    return doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_doc_num2(query):\n",
    "    query =  transformer.transform(vectorizer.transform(get_BOW(query)))\n",
    "    result={}\n",
    "    for x in range(term_matrix.shape[0]):\n",
    "         result[x]=cos_distance(query.toarray(),term_matrix[x].toarray())\n",
    "            \n",
    "    minvalue=1\n",
    "    first=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            first=item     \n",
    "    del result[first]\n",
    "    \n",
    "    minvalue=1\n",
    "    second=0\n",
    "    for item in result:\n",
    "        if minvalue > result[item]:\n",
    "            minvalue=result[item]\n",
    "            second=item     \n",
    "    return first,second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sentences from documents: 100%|██████████| 42/42 [12:31<00:00, 21.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "# store guessed question sentence no\n",
    "match_sent= [] #[[(best_match_sent_no,second_match_sent_no),...][second doc]]\n",
    "count = 0\n",
    "\n",
    "for dev_doc in tqdm(dev, desc='Extracting sentences from documents'):\n",
    "    count += 1\n",
    "    doc_match_sent = []\n",
    "    term_matrix = transformer.fit_transform(vectorizer.fit_transform(cal_BOW(dev_doc)))\n",
    "    for qa in dev_doc['qa']:\n",
    "        doc_match_sent.append(get_best_doc_num2(qa['question']))\n",
    "    match_sent.append(doc_match_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of set(mentioned_sent_number in the guessed )\n",
    "mentioned_sent = []\n",
    "\n",
    "for doc in match_sent:\n",
    "    tmp_doc = set()\n",
    "    for first,second in doc:\n",
    "        tmp_doc.add(first)\n",
    "        tmp_doc.add(second)\n",
    "    mentioned_sent.append(tmp_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sent(sentence):\n",
    "    sentence = sentence.split(\".\")[0]\n",
    "    tmp = []\n",
    "    tmp += sentence.split(\",\")\n",
    "    sentence  = tmp[0].split()\n",
    "    for fraction  in tmp[1:]:\n",
    "        sentence.append(\",\")\n",
    "        sentence.extend(fraction.split())\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "# st = StanfordNERTagger('/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "#                '/Users/ZhangJiaWei/Downloads/stanford-ner-2016-10-31/stanford-ner.jar') \n",
    "st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "               '/usr/share/stanford-ner/stanford-ner.jar') \n",
    "\n",
    "# tokenize sentence\n",
    "test_tag = []\n",
    "for i in range(len(dev)):\n",
    "    test_sent_tag = []\n",
    "    for j in range(len(dev[i]['sentences'])):\n",
    "        if j in mentioned_sent[i]:# mentioned in the guess\n",
    "            test_sent_tag.append(word_tokenizer.tokenize(dev[i]['sentences'][j]))\n",
    "#             test_sent_tag.append(split_sent(dev[i]['sentences'][j]))\n",
    "        else:\n",
    "            test_sent_tag.append([])\n",
    "    test_sent_tag = st.tag_sents(test_sent_tag)\n",
    "    test_tag.append(test_sent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn O and ORGANIZATION into other\n",
    "def tune_other(tag_list):\n",
    "    for i in range(len(tag_list)): # each document\n",
    "        for j in range(len(tag_list[i])): # each sentence\n",
    "            for k in range(len(tag_list[i][j])): # each question\n",
    "                term,tag = tag_list[i][j][k] \n",
    "                if term!='' and (tag == \"ORGANIZATION\"  or (len(term)>0 and (term,tag)!=tag_list[i][j][0] and tag == 'O' and term[0].isupper())):\n",
    "                    tag_list[i][j][k] = (term,\"OTHER\")\n",
    "\n",
    "tune_other(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def combine_entity(tag_list):\n",
    "    for k in range(len(tag_list)):\n",
    "        for i in range(len(tag_list[k])):\n",
    "            j = 0\n",
    "            while j < len(tag_list[k][i])-2:\n",
    "                term,tag = tag_list[k][i][j]\n",
    "                term_n,tag_n = tag_list[k][i][j+1]\n",
    "                if tag == tag_n and tag != \"O\" :\n",
    "                    if term_n != \",\" and  term_n !=\"%\":\n",
    "                        temp =  (term + \" \" + term_n,tag)\n",
    "                    else:\n",
    "                        temp =  (term + term_n,tag)\n",
    "                    tag_list[k][i][j] = temp\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 1\n",
    "                if term == \"-\" or term.encode('utf8') == \"\\xe2\\x80\\x93\" or term == \"\\x80\\x93\":\n",
    "                    temp =  (tag_list[k][i][j-1][0] + term + term_n,tag_list[k][i][j-1][1])\n",
    "                    tag_list[k][i][j-1] = temp\n",
    "                    del tag_list[k][i][j]\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 2\n",
    "                if term == \",\" and tag_list[k][i][j-1][0].isdigit() and tag_list[k][i][j+1][0].isdigit():\n",
    "                    temp =  (tag_list[k][i][j-1][0] + term + term_n,tag_list[k][i][j-1][1])\n",
    "                    tag_list[k][i][j-1] = temp\n",
    "                    del tag_list[k][i][j]\n",
    "                    del tag_list[k][i][j+1]\n",
    "                    j -= 2\n",
    "                j += 1\n",
    "\n",
    "combine_entity(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn O and ORGANIZATION into other\n",
    "def tune_other_2(tag_list):\n",
    "    for i in range(len(tag_list)): # each document\n",
    "        for j in range(len(tag_list[i])): # each sentence\n",
    "            for k in range(len(tag_list[i][j])): # each question\n",
    "                term,tag = tag_list[i][j][k] \n",
    "                if \"-\" in term and tag ==\"O\":\n",
    "                    tag_list[i][j][k] = (term,\"OTHER\")\n",
    "\n",
    "tune_other_2(test_tag)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "tags = [\"PERSON\",\"LOCATION\",\"NUMBER\",\"OTHER\"]\n",
    "entity_pool_1 = []\n",
    "for k in tqdm(test_tag, desc='Extracting entities'):\n",
    "    entity_doc_pool = []\n",
    "    for i in k:\n",
    "        sent_tag_dict = dict.fromkeys(tags,[])\n",
    "        for j in range(len(i)):\n",
    "            term,tag = i[j]\n",
    "            if tag == \"PERSON\" or tag == \"LOCATION\" or tag == \"OTHER\":\n",
    "                sent_tag_dict[tag] = sent_tag_dict[tag]+ [term]\n",
    "            elif tag == \"DATE\" or tag == \"TIME\" or tag == \"PERCENT\" or hasNumbers(term):\n",
    "                sent_tag_dict[\"NUMBER\"] = sent_tag_dict[\"NUMBER\"]+ [term]\n",
    "        entity_doc_pool.append(sent_tag_dict)\n",
    "    entity_pool_1.append(entity_doc_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_entity(first,second):\n",
    "    sent_tag_dict = dict.fromkeys(tags,[])\n",
    "    for k in [first,second]:\n",
    "        for j in test_tag[i][k]:\n",
    "            term,tag = j\n",
    "            if term =='':\n",
    "                continue\n",
    "            if tag == \"PERSON\" or tag == \"LOCATION\" or tag == \"OTHER\":\n",
    "                sent_tag_dict[tag] = sent_tag_dict[tag]+ [term]\n",
    "            elif tag == \"DATE\" or tag == \"TIME\" or tag == \"PERCENT\" or hasNumbers(term):\n",
    "                sent_tag_dict[\"NUMBER\"] = sent_tag_dict[\"NUMBER\"]+ [term]\n",
    "    return sent_tag_dict\n",
    "\n",
    "def remove_tag(list_tag):\n",
    "    list_tmp = []\n",
    "    for tup in list_tag:\n",
    "        term,tag = tup\n",
    "        if term != '':\n",
    "            list_tmp.append(term)\n",
    "    return list_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = []\n",
    "entity_pool = []\n",
    "tags = [\"PERSON\",\"LOCATION\",\"NUMBER\",\"OTHER\"]\n",
    "\n",
    "for i in range(len(match_sent)):\n",
    "    doc = dev[i]\n",
    "    sent_pool = doc['sentences']\n",
    "    test_list_tmp = []\n",
    "    entity_doc_pool = []\n",
    "    \n",
    "    for first,second in match_sent[i]:\n",
    "        tmp =  test_tag[i][first]+test_tag[i][second]\n",
    "        test_list_tmp.append(remove_tag(tmp))\n",
    "        entity_doc_pool.append(create_entity(first,second))\n",
    "        \n",
    "    test_list.append(test_list_tmp)\n",
    "    entity_pool.append(entity_doc_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import nltk\n",
    "\n",
    "# A simple rule-based question type classifier based on key words \n",
    "def get_question_type(question):\n",
    "    # TODO: HAND-CODED, NEED TO BE REFINED!!\n",
    "    # TODO: need to low-case to compare?\n",
    "    \n",
    "    type_rules = [\n",
    "        ('PERSON', [\"Who\", \"Whose\", \"Whom\", \"whom\"]),\n",
    "        ('LOCATION', [\"Where\"]),\n",
    "        ('NUMBER', [\"When\", \"few\", \"little\", \"much\", \"many\",\"size\",\n",
    "                   \"young\", \"old\", \"long\", \"year\", \"years\", \"day\"])\n",
    "    ]\n",
    "    \n",
    "    q_type = None\n",
    "    for question_type, key_words in type_rules:\n",
    "        if q_type == None:\n",
    "            for key_word in key_words:\n",
    "                if key_word in question:\n",
    "                    q_type = question_type\n",
    "                    break\n",
    "    if q_type == None:\n",
    "        q_type = 'OTHER'\n",
    "    # log for error analysis\n",
    "#     output_file.write('Q_type: ' + '\\t' + q_type + '\\n')\n",
    "    return q_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# among entities of the same type, the prefered entity should be \n",
    "# the one which is closer in the sentence to a open-class word\n",
    "# from the question.\n",
    "# ----> nouns, verbs, adjectives, and adverbs.\n",
    "def get_preferred_entity(entity_list, sentence, question):\n",
    "    preferred_entity = None\n",
    "    sentence_text = sentence\n",
    "#     question_text = word_tokenize(question)\n",
    "    question_text = split_sent(question)\n",
    "    sentence_tag = nltk.pos_tag(sentence_text,tagset='universal')\n",
    "    question_tag = nltk.pos_tag(question_text,tagset='universal')\n",
    "    \n",
    "    # initialize a list for comparing, and set all elements as 0\n",
    "    is_open_word = [0] * len(sentence_text)\n",
    "    # find an open word in the question\n",
    "    for word, tag in question_tag:\n",
    "        if tag in ['ADJ','NOUN','VERB','ADV']:\n",
    "            # if the open word appears in the sentence, then mark as 1\n",
    "            for i in range(len(sentence_text)):\n",
    "                if sentence_text[i] == word:\n",
    "                    is_open_word[i] = 1\n",
    "    \n",
    "    # find the closest distance to an open-class word for an entity\n",
    "    def get_distance(entity):\n",
    "        # get the position of entity, and find the open class word \n",
    "        # from the nearest at both sides\n",
    "        distance = None\n",
    "        position = sentence_text.index(entity)\n",
    "        for i in range(1, len(sentence_text)):\n",
    "            if position - i >= 0:\n",
    "                if is_open_word[position - i] == 1:  # find an open-class word on the left\n",
    "                    distance = i\n",
    "                    break\n",
    "                elif position + i < len(is_open_word):  # find an open-class word on the right\n",
    "                    if is_open_word[position + i] == 1:\n",
    "                        distance = i\n",
    "                        break\n",
    "                else:\n",
    "                    distance = len(sentence_text) + 1  # didn't find open-class words\n",
    "        return distance\n",
    "    \n",
    "    # get distance for each entity and choose the best one\n",
    "    all_distance = []\n",
    "    for entity in entity_list:\n",
    "        all_distance.append(get_distance(entity))\n",
    "        preferred_entity = entity_list[all_distance.index(min(all_distance))]\n",
    "\n",
    "    return preferred_entity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ranked_ans(entities_dic, question, sentence):\n",
    "    # identify if the entity set is empty. If True, return nothing\n",
    "    is_empty = True\n",
    "    for values in entities_dic.values():\n",
    "        if len(values) != 0:\n",
    "            is_empty = False\n",
    "            \n",
    "    if is_empty == False:\n",
    "        q_type = get_question_type(question)\n",
    "        tmp_rank = {}\n",
    "        for ent_type,entities in entities_dic.items():\n",
    "            # answers whose content words all appear in the question should be ranked lowest.\n",
    "            for entity in entities:\n",
    "                tmp_rank[entity] = tmp_rank.setdefault(entity,0)\n",
    "                if entity in question:\n",
    "                    tmp_rank[entity] = tmp_rank.setdefault(entity,0) - 1\n",
    "            # Answers which match the question type should be ranked higher than those that don't\n",
    "            if ent_type == q_type and ent_type != 'OTHER':\n",
    "                for entity in entities:\n",
    "                    tmp_rank[entity] = tmp_rank.setdefault(entity,0) + 1\n",
    "                ######## TODO: Apply this to all types?\n",
    "            # entity closer in the sentence to a closed-class word should be preferred\n",
    "            preferred_entity = get_preferred_entity(entities, sentence, question)\n",
    "            if preferred_entity != None:\n",
    "                tmp_rank[preferred_entity] = tmp_rank.setdefault(preferred_entity,0) + 1\n",
    "        # sort and choose the best answer\n",
    "        sorted_ans = sorted(tmp_rank.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        # print sorted_ans\n",
    "        # log for error analysis\n",
    "#         output_file.write('Ranked Answers: ' + '\\t' + str(sorted_ans).encode('utf-8') + '\\n\\n')\n",
    "        # TODO: bug here. list out of index??? why?\n",
    "        if len(sorted_ans) != 0:\n",
    "            best_ans = sorted_ans[0][0]\n",
    "        else:\n",
    "            best_ans = ''\n",
    "        return best_ans\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "num = 0\n",
    "count = 1\n",
    "correct_sum = 0\n",
    "corr_sen_retr_count = 0\n",
    "\n",
    "with open(\"result.txt\",'w') as output_file:\n",
    "    for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "        for j in range(len(match_sent[i])):\n",
    "            result = get_ranked_ans(entity_pool[i][j], dev[i][\"qa\"][j]['question'], test_list[i][j])\n",
    "            output_file.write('Retrieved Entities: ' + '\\t' + str(entity_pool[i][j]) + '\\n\\n')\n",
    "#             q_id = dev[i][\"qa\"][j]['id']\n",
    "            count += 1\n",
    "            cor_answer = dev[i][\"qa\"][j]['answer']\n",
    "            Q = dev[i][\"qa\"][j]['question']\n",
    "            A_sentence = dev[i][\"sentences\"][dev[i][\"qa\"][j]['answer_sentence']]\n",
    "            sent_1, sent_2 = match_sent[i][j]\n",
    "            guessed_sentence = dev[i]['sentences'][sent_1] + ' ' + dev[i]['sentences'][sent_2]\n",
    "            \n",
    "            if result == cor_answer:\n",
    "                correct_sum += 1\n",
    "            else:\n",
    "                string1 = 'Retrieved Sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string1_1 = '==== WRONG SENTENCES! ==== \\n' + 'Guessed_Sentence: ' + '\\t' + guessed_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string1_2 = 'CORRECT_Sentence: ' + '\\t' + A_sentence.encode('utf-8')+\"\\n\\n\"\n",
    "                string2 = 'Q: ' + '\\t' + Q.encode('utf-8') + '\\n\\n'\n",
    "                string3 = 'CORRECT_ANSWER: ' + '\\t' + cor_answer.encode('utf-8') + '\\n'\n",
    "                string4 = 'GUESSED_ANSWER: ' + '\\t' + result.encode('utf-8')+\"\\n\"\n",
    "                \n",
    "                if A_sentence not in guessed_sentence:\n",
    "                    output_file.write(string1_1)\n",
    "                    output_file.write(string1_2)\n",
    "                else:\n",
    "                    corr_sen_retr_count += 1\n",
    "                    output_file.write(string1)\n",
    "                output_file.write(string2)\n",
    "                output_file.write('='*60 + '\\n')\n",
    "                output_file.write(string3)\n",
    "                output_file.write(string4)\n",
    "                output_file.write('='*60 + '\\n\\n')\n",
    "    print 'correct sum: ' + str(correct_sum)\n",
    "    print 'Sentence Recall: ' + str((corr_sen_retr_count+0.0)/count)\n",
    "    \n",
    "for i in dev:\n",
    "    for j in i[\"qa\"]:\n",
    "        num += 1\n",
    "print (correct_sum+0.0)/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answering:   0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Answering:   2%|▏         | 1/42 [00:03<02:40,  3.92s/it]\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Answering:   5%|▍         | 2/42 [00:10<03:05,  4.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:   7%|▋         | 3/42 [00:11<02:18,  3.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  10%|▉         | 4/42 [00:12<01:44,  2.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  12%|█▏        | 5/42 [00:13<01:27,  2.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  14%|█▍        | 6/42 [00:16<01:29,  2.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  17%|█▋        | 7/42 [00:19<01:32,  2.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  19%|█▉        | 8/42 [00:21<01:28,  2.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  21%|██▏       | 9/42 [00:25<01:35,  2.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  24%|██▍       | 10/42 [00:27<01:25,  2.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  26%|██▌       | 11/42 [00:30<01:20,  2.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  29%|██▊       | 12/42 [00:32<01:13,  2.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  31%|███       | 13/42 [00:34<01:10,  2.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  33%|███▎      | 14/42 [00:37<01:09,  2.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  36%|███▌      | 15/42 [00:37<00:53,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  38%|███▊      | 16/42 [00:41<01:01,  2.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  40%|████      | 17/42 [00:45<01:10,  2.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  43%|████▎     | 18/42 [00:47<01:02,  2.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  45%|████▌     | 19/42 [00:50<01:07,  2.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  48%|████▊     | 20/42 [00:52<00:58,  2.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  50%|█████     | 21/42 [00:54<00:49,  2.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  52%|█████▏    | 22/42 [00:59<01:04,  3.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  55%|█████▍    | 23/42 [01:03<01:03,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  57%|█████▋    | 24/42 [01:08<01:09,  3.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  60%|█████▉    | 25/42 [01:11<01:00,  3.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  62%|██████▏   | 26/42 [01:15<00:58,  3.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  64%|██████▍   | 27/42 [01:19<00:56,  3.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  67%|██████▋   | 28/42 [01:24<00:57,  4.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  69%|██████▉   | 29/42 [01:27<00:49,  3.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  71%|███████▏  | 30/42 [01:27<00:34,  2.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  74%|███████▍  | 31/42 [01:29<00:27,  2.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  76%|███████▌  | 32/42 [01:33<00:30,  3.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  79%|███████▊  | 33/42 [01:40<00:36,  4.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  81%|████████  | 34/42 [01:41<00:26,  3.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  83%|████████▎ | 35/42 [01:44<00:21,  3.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  86%|████████▌ | 36/42 [01:47<00:19,  3.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  88%|████████▊ | 37/42 [01:49<00:13,  2.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  90%|█████████ | 38/42 [01:50<00:08,  2.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  93%|█████████▎| 39/42 [01:55<00:09,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  95%|█████████▌| 40/42 [01:56<00:05,  2.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering:  98%|█████████▊| 41/42 [02:06<00:04,  4.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "Answering: 100%|██████████| 42/42 [02:07<00:00,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# run on test data\n",
    "\n",
    "with open(\"result.txt\",'w') as output_file:\n",
    "    output_file.write('id,answer'+'\\n')\n",
    "    for i in tqdm(range(len(match_sent)), desc='Answering'):\n",
    "        for j in range(len(match_sent[i])):\n",
    "            result = get_ranked_ans(entity_pool[i][j], dev[i][\"qa\"][j]['question'], test_list[i][j])\n",
    "            result = result.encode('utf-8')\n",
    "            reuslt = result.replace('\" ','')\n",
    "            result = result.replace('\"','')\n",
    "            result = result.replace(\",\",\"-COMMA-\")\n",
    "            q_id = dev[i][\"qa\"][j]['id']\n",
    "            output_file.write(str(q_id) + ',' + str(result) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
